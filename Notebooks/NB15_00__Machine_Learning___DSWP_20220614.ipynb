{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "NB15_00__Machine_Learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luciorgomes/DSWP/blob/master/Notebooks/NB15_00__Machine_Learning___DSWP_20220614.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShVXyGj9wkgN"
      },
      "source": [
        "<center><h1><b><i>MACHINE LEARNING WITH PYTHON</i></b></h1></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYQ4cDfcPu4e"
      },
      "source": [
        "___\n",
        "# **NOTAS E OBSERVAÇÕES**\n",
        "* Abordar o impacto do desbalanceamento da amostra;\n",
        "* Colocar AUROC no material e mostrar o cut off para classificação entre 0 e 1;\n",
        "* Conceitos estatísticos de bias & variance;\n",
        "* Ver Sklearn.optimize: https://web.telegram.org/#/im?p=g497957288;\n",
        "* Construir a package para conter todas as funções definidas e colocar estas funções na package --> Manutenção rápida, fácil e centralizada! Desta forma, o tópico (\"Funções usadas neste tutorial\") vai totalmente para o package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YvhLC_uf4_G"
      },
      "source": [
        "___\n",
        "# **AGENDA**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgX6n2VDyY1O"
      },
      "source": [
        "___\n",
        "# **REFERÊNCIAS**\n",
        "* [scikit-learn - Machine Learning With Python](https://scikit-learn.org/stable/);\n",
        "* [An Introduction to Machine Learning Theory and Its Applications: A Visual Tutorial with Examples](https://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer)\n",
        "* [The Difference Between Artificial Intelligence, Machine Learning, and Deep Learning](https://medium.com/iotforall/the-difference-between-artificial-intelligence-machine-learning-and-deep-learning-3aa67bff5991)\n",
        "* [A Gentle Guide to Machine Learning](https://blog.monkeylearn.com/a-gentle-guide-to-machine-learning/)\n",
        "* [A Visual Introduction to Machine Learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
        "* [Introduction to Machine Learning](http://alex.smola.org/drafts/thebook.pdf)\n",
        "* [The 10 Statistical Techniques Data Scientists Need to Master](https://medium.com/cracking-the-data-science-interview/the-10-statistical-techniques-data-scientists-need-to-master-1ef6dbd531f7)\n",
        "* [Tune: a library for fast hyperparameter tuning at any scale](https://towardsdatascience.com/fast-hyperparameter-tuning-at-scale-d428223b081c)\n",
        "* [How to lie with Data Science](https://towardsdatascience.com/how-to-lie-with-data-science-5090f3891d9c)\n",
        "* [5 Reasons “Logistic Regression” should be the first thing you learn when becoming a Data Scientist](https://towardsdatascience.com/5-reasons-logistic-regression-should-be-the-first-thing-you-learn-when-become-a-data-scientist-fcaae46605c4)\n",
        "* [Machine learning on categorical variables](https://towardsdatascience.com/machine-learning-on-categorical-variables-3b76ffe4a7cb)\n",
        "\n",
        "## Deep Learning & Neural Networks\n",
        "\n",
        "- [An Introduction to Neural Networks](http://www.cs.stir.ac.uk/~lss/NNIntro/InvSlides.html)\n",
        "- [An Introduction to Image Recognition with Deep Learning](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721)\n",
        "- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsCbZd2epfxo"
      },
      "source": [
        "___\n",
        "# **INTRODUÇÃO**\n",
        "\n",
        "* \"__Information is the oil of the 21st century, and analytics is the combustion engine__.\" - Peter Sondergaard, SVP, Garner Research;\n",
        "\n",
        "\n",
        ">O foco deste capítulo será:\n",
        "* Linear, Logistic Regression, Decision Tree, Random Forest, Support Vector Machine and XGBoost algorithms for building Machine Learning models;\n",
        "* Entender como resolver problemas de classificação e Regressão;\n",
        "* Aplicar técnicas de Ensemble como Bagging e Boosting;\n",
        "* Como medir a acurácia dos modelos de Machine Learning;\n",
        "* Aprender os principais algoritmos de Machine Learning tanto das técnicas de aprendizagem supervisionada quanto da não-supervisionada.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqqB2vaHXMGt"
      },
      "source": [
        "___\n",
        "# **ARTIFICIAL INTELLIGENCE VS MACHINE LEARNING VS DEEP LEARNING**\n",
        "* **Machine Learning** - dá aos computadores a capacidade de aprender sem serem explicitamente programados. Os computadores podem melhorar sua capacidade de aprendizagem através da prática de uma tarefa, geralmente usando grandes conjuntos de dados.\n",
        "* **Deep Learning** - é um método de Machine Learning que depende de redes neurais artificiais, permitindo que os sistemas de computadores aprendam pelo exemplo, assim como nós humanos aprendemos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P961GcguXFFA"
      },
      "source": [
        "![EvolutionOfAI](https://github.com/MathMachado/Materials/blob/master/Evolution%20of%20AI.PNG?raw=true)\n",
        "\n",
        "Source: [Artificial Intelligence vs. Machine Learning vs. Deep Learning](https://github.com/MathMachado/P4ML/blob/DS_Python/Material/Evolution%20of%20AI.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkqGtO88ZkPr"
      },
      "source": [
        "![AI_vs_ML_vs_DL](https://github.com/MathMachado/Materials/blob/master/AI_vs_ML_vs_DL.PNG?raw=true)\n",
        "\n",
        "Source: [Artificial Intelligence vs. Machine Learning vs. Deep Learning](https://towardsdatascience.com/artificial-intelligence-vs-machine-learning-vs-deep-learning-2210ba8cc4ac)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xesQpzfmaqj6"
      },
      "source": [
        "![ML_vs_DL](https://github.com/MathMachado/Materials/blob/master/ML_vs_DL.PNG?raw=true)\n",
        "\n",
        "Source: [Artificial Intelligence vs. Machine Learning vs. Deep Learning](https://towardsdatascience.com/artificial-intelligence-vs-machine-learning-vs-deep-learning-2210ba8cc4ac)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeIVR59IIS7f"
      },
      "source": [
        "___\n",
        "# **MACHINE LEARNING - TECHNIQUES**\n",
        "\n",
        "* Supervised Learning\n",
        "* Unsupervised Learning\n",
        "\n",
        "![MachineLearning](https://github.com/MathMachado/Materials/blob/master/MachineLearningTechniques.jpg?raw=true)\n",
        "\n",
        "Source: [Machine Learning for Everyone](https://vas3k.com/blog/machine_learning/?source=post_page-----885aa35db58b----------------------)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvwp5UHdBiup"
      },
      "source": [
        "___\n",
        "# **NOSSO FOCO AQUI SERÁ...**\n",
        "\n",
        "![ClassicalML](https://github.com/MathMachado/Materials/blob/master/ClassicalML.jpg?raw=true)\n",
        "\n",
        "Source: [Machine Learning for Everyone](https://vas3k.com/blog/machine_learning/?source=post_page-----885aa35db58b----------------------)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBLSvJTXHBjK"
      },
      "source": [
        "___\n",
        "# **CHEETSHEET**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdjR3nahUuKq"
      },
      "source": [
        "\n",
        "![Scikit-Learn](https://github.com/MathMachado/Materials/blob/master/scikit-learn-1.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRukccWQSklx"
      },
      "source": [
        "## Medidas para avaliarmos a variabilidade presente nos dados\n",
        "* As principais medidas para medirmos a variabilidade dos dados são amplitude, variância, desvio padrão e coeficiente de variação;\n",
        "* Estas medidas nos permite concluir se os dados são homogêneos (menor dispersão/variabilidade) ou heterogêneos (maior variabilidade/dispersão).\n",
        "\n",
        "* **Na próxima versão, trazer estes conceitos para o Notebook e usar o Python para calcular estas medidas**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBR8tWV_lhQq"
      },
      "source": [
        "___\n",
        "# **ENSEMBLE METHODS** (= Combinar modelos preditivos)\n",
        "* Métodos\n",
        "    * **Bagging** (Bootstrap AGGregatING)\n",
        "    * **Boosting**\n",
        "    * Stacking --> Não é muito utilizado\n",
        "* Evita overfitting (Overfitting é quando o modelo/função se ajusta muito bem ao dados de treinamento, sendo ineficiente para generalizar para outras amostras/população).\n",
        "* Constroi meta-classificadores: combinar os resultados de vários algoritmos para produzir previsões mais precisas e robustas do que as previsões de cada classificador individual.\n",
        "* Ensemble reduz/minimiza os efeitos das principais causas de erros nos modelos de Machine Learning:\n",
        "    * ruído;\n",
        "    * bias (viés);\n",
        "    * variância --> Principal medida para medir a variabilidade presente nos dados.\n",
        "\n",
        "# Referências\n",
        "* [Simple guide for ensemble learning methods](https://towardsdatascience.com/simple-guide-for-ensemble-learning-methods-d87cc68705a2) - Explica didaticamente como funcionam ensembes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25RW8u-Sj780"
      },
      "source": [
        "### Leitura Adicional\n",
        "* [Ensemble methods: bagging, boosting and stacking](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)\n",
        "* [Ensemble Methods in Machine Learning: What are They and Why Use Them?](https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f)\n",
        "* [Ensemble Learning Using Scikit-learn](https://towardsdatascience.com/ensemble-learning-using-scikit-learn-85c4531ff86a)\n",
        "* [Let’s Talk About Machine Learning Ensemble Learning In Python](https://medium.com/fintechexplained/lets-talk-about-machine-learning-ensemble-learning-in-python-382747e5fba8)\n",
        "* [Boosting, Bagging, and Stacking — Ensemble Methods with sklearn and mlens](https://medium.com/@rrfd/boosting-bagging-and-stacking-ensemble-methods-with-sklearn-and-mlens-a455c0c982de)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FugME1HSl4jJ"
      },
      "source": [
        "___\n",
        "# **PARAMETER TUNNING** (= Parâmetros ótimos dos modelos de Machine Learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_147cIRl9F1"
      },
      "source": [
        "## GridSearch (Ferramenta ou meio que vamos utilizar para otimização dos parâmetros dos modelos de ML)\n",
        "* Encontra os parâmetros ótimos (hyperparameter tunning) que melhoram a acurácia dos modelos.\n",
        "* Necessita dos seguintes inputs:\n",
        "    * A matrix $X_{p}$ com as $p$ COLUNAS (variáveis ou atributos) do dataframe;\n",
        "    * A matriz $y_{p}$ com a COLUNA-target (vaiável resposta);\n",
        "    * Exemplo: DecisionTree, RandomForestClassifier, XGBoostClassificer e etc;\n",
        "    * Um dicionário com os parâmetros a serem otimizados;\n",
        "    * O número de folds para o método de Cross-validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39Sg77fbTWCO"
      },
      "source": [
        "___\n",
        "# **MODEL SELECTION & EVALUATION**\n",
        "> Nesta fase identificamos e aplicamos as melhores métricas (Accuracy, Sensitivity, Specificity, F-Score, AUC, R-Sq, Adj R-SQ, RMSE (Root Mean Square Error)) para avaliar o desempenho/acurácia/performance dos modelos de ML.\n",
        ">> Treinamos os modelos de ML usando a amostra de treinamento e avaliamos o desempenho/acurácia/performance na amostra de teste/validação.\n",
        "\n",
        "* Leitura Adicional\n",
        "    * [The 5 Classification Evaluation metrics every Data Scientist must know](https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226)\n",
        "    * [Confusion matrix and other metrics in machine learning](https://medium.com/hugo-ferreiras-blog/confusion-matrix-and-other-metrics-in-machine-learning-894688cb1c0a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQQVzZ2ZTYrB"
      },
      "source": [
        "## Confusion Matrix\n",
        "* Termos associados à Confusion Matrix:\n",
        "    * **Verdadeiro Positivo** (TP = True Positive): Quando o valor observado é True e o modelo estima como True. Ou seja, o modelo acertou na estimativa.\n",
        "        * Exemplo: **Observado**: Fraude (Positive); **Modelo**: Fraude (Positive) --> Modelo acertou!\n",
        "    * **Verdadeiro Negativo** (TN = True Negative): Quando o valor observado é False e o modelo estima como False. Ou seja, o modelo acertou na estimativa;\n",
        "        * Exemplo: **Observado**: NÃO-Fraude (Negative); **Modelo**: NÃO-Fraude (Negative) --> Modelo acertou!\n",
        "    * **Falso Positivo** (FP = False Positive): Quando o valor observado é False e o modelo estima como True. Ou seja, o modelo errou na estimativa. \n",
        "        * Exemplo: **Observado**: NÃO-Fraude (Negative); **Modelo**: Fraude (Positive) --> Modelo errou!\n",
        "    * **Falso Negativo** (FN = False Negative): Quando o valor observado é True e o modelo estima como False.\n",
        "        * Exemplo: **Observado**: Fraude (Positive); **Modelo**: NÃO-Fraude (Negative) --> Modelo errou!\n",
        "\n",
        "* Consulte [Confusion matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py)\n",
        "\n",
        "![ConfusionMatrix](https://github.com/MathMachado/Materials/blob/master/ConfusionMatrix.PNG?raw=true)\n",
        "\n",
        "Source: [Confusion Matrix](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781838555078/6/ch06lvl1sec34/confusion-matrix)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci-6eiqBTgbL"
      },
      "source": [
        "## Accuracy\n",
        "> Accuracy - é o número de previsões corretas feitas pelo modelo.\n",
        "\n",
        "Responde à seguinte pergunta:\n",
        "\n",
        "```\n",
        "Com que frequência o classificador (modelo preditivo) classifica corretamente?\n",
        "```\n",
        "\n",
        "$$Accuracy= \\frac{TP+TN}{TP+TN+FP+FN}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7YI8X5TRx-R"
      },
      "source": [
        "## Precision (ou Specificity)\n",
        "> **Precision** - fornece informações sobre o desempenho em relação a Falsos Positivos (quantos capturamos).\n",
        "\n",
        "Responde à seguinte pergunta:\n",
        "\n",
        "```\n",
        "Com relação ao resultado Positivo, com que frequência o classificador está correto?\n",
        "```\n",
        "\n",
        "\n",
        "$$Precision= \\frac{TP}{TP+FP}$$\n",
        "\n",
        "**Exemplo**: Precison nos dirá a proporção de clientes que o modelo estimou como sendo Fraude quando, na verdade, são fraude.\n",
        "\n",
        "**Comentário**: Se nosso foco é minimizar Falso Negativos (FN), então precisamos nos esforçar para termos Recall próximo de 100%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO39n8x_Sz3L"
      },
      "source": [
        "## Recall (ou Sensitivity)\n",
        "> **Recall** - nos fornece informações sobre o desempenho de um classificador em relação a Falsos Negativos (quantos perdemos).\n",
        "\n",
        "Responde à seguinte pergunta:\n",
        "\n",
        "```\n",
        "Quando o valor observado é Positivo, com que frequência o classificador está correto?\n",
        "```\n",
        "\n",
        "$$Recall = Sensitivity = \\frac{TP}{TP+FN}$$\n",
        "\n",
        "**Exemplo**: Recall é a proporção de clientes observados como Fraude e que o modelo estima como Fraude.\n",
        "\n",
        "**Comentário**: Se nosso foco for minimizar Falso Positivos (FP), então precisamos nos esforçar para fazer Precision mais próximo de 100% possível."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htS6rdHVVXRG"
      },
      "source": [
        "## Specificity\n",
        "> **Specificity** - proporção de TN por TN+FP.\n",
        "\n",
        "Responde à seguinte pergunta:\n",
        "\n",
        "```\n",
        "Quando o valor observado é Negativo, com que frequência o classificador está correto?\n",
        "```\n",
        "\n",
        "**Exemplo**: Specificity é a proporção de clientes NÃO-Fraude que o modelo estima como NÃO-Fraude.\n",
        "\n",
        "$$Specificity= \\frac{TN}{TN+FP}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNn0twadTacc"
      },
      "source": [
        "## F1-Score\n",
        "> F1-Score é a média harmônica entre Recall e Precision e é um número entre 0 e 1. Quanto mais próximo de 1, melhor. Quanto mais próximo de 0, pior. Ou seja, é um equilíbrio entre Recall e Precision.\n",
        "\n",
        "$$F1\\_Score= 2\\left(\\frac{Recall*Precision}{Recall+Precision}\\right)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkCubyUCP_hn"
      },
      "source": [
        "### Funções usadas neste tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD2pH9hfTnZv"
      },
      "source": [
        "#### Função para Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr8LczrSQB0x"
      },
      "source": [
        "def funcao_cross_val_score(modelo, X_treinamento, y_treinamento, CV):\n",
        "    # versão com sklearn.model_selection.cross_validate:\n",
        "    #a_scores_CV = cross_validate(modelo, X_treinamento, y_treinamento, cv = CV, scoring = metodo)\n",
        "    #print(f'Média das Acurácias calculadas pelo CV....: {100*round(a_scores_CV.mean(),4)}')\n",
        "    #print(f'std médio das Acurácias calculadas pelo CV: {100*round(a_scores_CV.std(),4)}')\n",
        "    #return a_scores_CV\n",
        "\n",
        "    #versão com cross_val_score::\n",
        "    a_scores_CV = cross_val_score(modelo, X_treinamento, y_treinamento, cv = CV)\n",
        "    print(f'Média das Acurácias calculadas pelo CV....: {100*round(a_scores_CV.mean(),4)}')\n",
        "    print(f'std médio das Acurácias calculadas pelo CV: {100*round(a_scores_CV.std(),4)}')\n",
        "    return a_scores_CV"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ROlyvgij2yl"
      },
      "source": [
        "#### Função para plotar a Confusion Matrix\n",
        "* Extraído de [Confusion Matrix Visualization](https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klQ0FLOIgeX1"
      },
      "source": [
        "def mostra_confusion_matrix(cf, \n",
        "                            group_names = None, \n",
        "                            categories = 'auto', \n",
        "                            count = True, \n",
        "                            percent = True, \n",
        "                            cbar = True, \n",
        "                            xyticks = False, \n",
        "                            xyplotlabels = True, \n",
        "                            sum_stats = True, \n",
        "                            figsize = (8, 8), \n",
        "                            cmap = 'Blues'):\n",
        "    '''\n",
        "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
        "    Arguments\n",
        "    ---------\n",
        "    cf:            confusion matrix to be passed in\n",
        "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
        "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
        "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
        "    normalize:     If True, show the proportions for each category. Default is True.\n",
        "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
        "                   Default is True.\n",
        "    xyticks:       If True, show x and y ticks. Default is True.\n",
        "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
        "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
        "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
        "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
        "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
        "    '''\n",
        "\n",
        "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
        "    blanks = ['' for i in range(cf.size)]\n",
        "\n",
        "    if group_names and len(group_names)==cf.size:\n",
        "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
        "    else:\n",
        "        group_labels = blanks\n",
        "\n",
        "    if count:\n",
        "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
        "    else:\n",
        "        group_counts = blanks\n",
        "\n",
        "    if percent:\n",
        "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
        "    else:\n",
        "        group_percentages = blanks\n",
        "\n",
        "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
        "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
        "\n",
        "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
        "    if sum_stats:\n",
        "        #Accuracy is sum of diagonal divided by total observations\n",
        "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
        "\n",
        "        #if it is a binary confusion matrix, show some more stats\n",
        "        if len(cf)==2:\n",
        "            #Metrics for Binary Confusion Matrices\n",
        "            precision = cf[1,1] / sum(cf[:,1])\n",
        "            recall    = cf[1,1] / sum(cf[1,:])\n",
        "            f1_score  = 2*precision*recall / (precision + recall)\n",
        "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(accuracy,precision,recall,f1_score)\n",
        "        else:\n",
        "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
        "    else:\n",
        "        stats_text = \"\"\n",
        "\n",
        "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
        "    if figsize==None:\n",
        "        #Get default figure size if not set\n",
        "        figsize = plt.rcParams.get('figure.figsize')\n",
        "\n",
        "    if xyticks==False:\n",
        "        #Do not show categories if xyticks is False\n",
        "        categories=False\n",
        "\n",
        "    # MAKE THE HEATMAP VISUALIZATION\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
        "\n",
        "    if xyplotlabels:\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label' + stats_text)\n",
        "    else:\n",
        "        plt.xlabel(stats_text)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J-sTUfTTdLi"
      },
      "source": [
        "#### Função para o GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap3WMXqDthu9"
      },
      "source": [
        "def GridSearchOptimizer(modelo, ml_Opt, d_Parametros, X_treinamento, y_treinamento, X_teste, y_teste, i_CV, l_colunas):\n",
        "    ml_GridSearchCV = GridSearchCV(modelo, d_Parametros, cv = i_CV, n_jobs = -1, verbose= 10, scoring = 'accuracy')\n",
        "    start = time()\n",
        "    ml_GridSearchCV.fit(X_treinamento, y_treinamento)\n",
        "    tempo_elapsed = time()-start\n",
        "    #print(f\"\\nGridSearchCV levou {tempo_elapsed:.2f} segundos.\")\n",
        "\n",
        "    # Parâmetros que otimizam a classificação:\n",
        "    print(f'\\nParametros otimizados: {ml_GridSearchCV.best_params_}')\n",
        "    \n",
        "    if ml_Opt == 'ml_DT2':\n",
        "        print(f'\\nDecisionTreeClassifier *********************************************************************************************************')\n",
        "        ml_Opt = DecisionTreeClassifier(criterion= ml_GridSearchCV.best_params_['criterion'], \n",
        "                                        max_depth= ml_GridSearchCV.best_params_['max_depth'],\n",
        "                                        max_leaf_nodes= ml_GridSearchCV.best_params_['max_leaf_nodes'],\n",
        "                                        min_samples_split= ml_GridSearchCV.best_params_['min_samples_leaf'],\n",
        "                                        min_samples_leaf= ml_GridSearchCV.best_params_['min_samples_split'], \n",
        "                                        random_state= i_Seed)\n",
        "        \n",
        "    elif ml_Opt == 'ml_RF2':\n",
        "        print(f'\\nRandomForestClassifier *********************************************************************************************************')\n",
        "        ml_Opt = RandomForestClassifier(bootstrap= ml_GridSearchCV.best_params_['bootstrap'], \n",
        "                                        max_depth= ml_GridSearchCV.best_params_['max_depth'],\n",
        "                                        max_features= ml_GridSearchCV.best_params_['max_features'],\n",
        "                                        min_samples_leaf= ml_GridSearchCV.best_params_['min_samples_leaf'],\n",
        "                                        min_samples_split= ml_GridSearchCV.best_params_['min_samples_split'],\n",
        "                                        n_estimators= ml_GridSearchCV.best_params_['n_estimators'],\n",
        "                                        random_state= i_Seed)\n",
        "        \n",
        "    elif ml_Opt == 'ml_AB2':\n",
        "        print(f'\\nAdaBoostClassifier *********************************************************************************************************')\n",
        "        ml_Opt = AdaBoostClassifier(algorithm='SAMME.R', \n",
        "                                    base_estimator=RandomForestClassifier(bootstrap = False, \n",
        "                                                                          max_depth = 10, \n",
        "                                                                          max_features = 'auto', \n",
        "                                                                          min_samples_leaf = 1, \n",
        "                                                                          min_samples_split = 2, \n",
        "                                                                          n_estimators = 400), \n",
        "                                    learning_rate = ml_GridSearchCV.best_params_['learning_rate'], \n",
        "                                    n_estimators = ml_GridSearchCV.best_params_['n_estimators'], \n",
        "                                    random_state = i_Seed)\n",
        "        \n",
        "    elif ml_Opt == 'ml_GB2':\n",
        "        print(f'\\nGradientBoostingClassifier *********************************************************************************************************')\n",
        "        ml_Opt = GradientBoostingClassifier(learning_rate = ml_GridSearchCV.best_params_['learning_rate'], \n",
        "                                            n_estimators = ml_GridSearchCV.best_params_['n_estimators'], \n",
        "                                            max_depth = ml_GridSearchCV.best_params_['max_depth'], \n",
        "                                            min_samples_split = ml_GridSearchCV.best_params_['min_samples_split'], \n",
        "                                            min_samples_leaf = ml_GridSearchCV.best_params_['min_samples_leaf'], \n",
        "                                            max_features = ml_GridSearchCV.best_params_['max_features'])\n",
        "        \n",
        "    elif ml_Opt == 'ml_XGB2':\n",
        "        print(f'\\nXGBoostingClassifier *********************************************************************************************************')\n",
        "        ml_Opt = XGBoostingClassifier(learning_rate= ml_GridSearchCV.best_params_['learning_rate'], \n",
        "                                      max_depth= ml_GridSearchCV.best_params_['max_depth'], \n",
        "                                      colsample_bytree= ml_GridSearchCV.best_params_['colsample_bytree'], \n",
        "                                      subsample= ml_GridSearchCV.best_params_['subsample'], \n",
        "                                      gamma= ml_GridSearchCV.best_params_['gamma'], \n",
        "                                      min_child_weight= ml_GridSearchCV.best_params_['min_child_weight'])\n",
        "        \n",
        "    # Treina novamente usando os parametros otimizados...\n",
        "    ml_Opt.fit(X_treinamento, y_treinamento)\n",
        "\n",
        "    # Cross-Validation com 10 folds\n",
        "    print(f'\\n********* CROSS-VALIDATION ***********')\n",
        "    a_scores_CV = funcao_cross_val_score(ml_Opt, X_treinamento, y_treinamento, i_CV)\n",
        "\n",
        "    # Faz predições com os parametros otimizados...\n",
        "    y_pred = ml_Opt.predict(X_teste)\n",
        "  \n",
        "    # Importância das COLUNAS\n",
        "    print(f'\\n********* IMPORTÂNCIA DAS COLUNAS ***********')\n",
        "    df_importancia_variaveis = pd.DataFrame(zip(l_colunas, ml_Opt.feature_importances_), columns= ['coluna', 'importancia'])\n",
        "    df_importancia_variaveis = df_importancia_variaveis.sort_values(by= ['importancia'], ascending=False)\n",
        "    print(df_importancia_variaveis)\n",
        "\n",
        "    # Matriz de Confusão\n",
        "    print(f'\\n********* CONFUSION MATRIX - PARAMETER TUNNING ***********')\n",
        "    cf_matrix = confusion_matrix(y_teste, y_pred)\n",
        "    cf_labels = ['True_Negative', 'False_Positive', 'False_Negative', 'True_Positive']\n",
        "    cf_categories = ['Zero', 'One']\n",
        "    mostra_confusion_matrix(cf_matrix, group_names = cf_labels, categories = cf_categories)\n",
        "\n",
        "    return ml_Opt, ml_GridSearchCV.best_params_"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMnQn2XgT7Mg"
      },
      "source": [
        "#### Função para selecionar COLUNAS relevantes dos dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsnHcaeLUDFS"
      },
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "def seleciona_colunas_relevantes(modelo, X_treinamento, X_teste, threshold = 0.05):\n",
        "    # Cria um seletor para selecionar as COLUNAS com importância > threshold\n",
        "    sfm = SelectFromModel(modelo, threshold)\n",
        "    \n",
        "    # Treina o seletor\n",
        "    sfm.fit(X_treinamento, y_treinamento)\n",
        "\n",
        "    # Mostra o indice das COLUNAS mais importantes\n",
        "    print(f'\\n********** COLUNAS Relevantes ******')\n",
        "    print(sfm.get_support(indices=True))\n",
        "\n",
        "    # Seleciona somente as COLUNAS relevantes\n",
        "    X_treinamento_I = sfm.transform(X_treinamento)\n",
        "    X_teste_I = sfm.transform(X_teste)\n",
        "    return X_treinamento_I, X_teste_I   "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd98JFSGUV5n"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e0m7lEnYOV9"
      },
      "source": [
        "### Função para calcular a importância das colunas/variáveis/atributos\n",
        "* Source: [Plotting Feature Importances](https://www.kaggle.com/grfiv4/plotting-feature-importances)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjco0HnNYr-N"
      },
      "source": [
        "def mostra_feature_importances(clf, X_treinamento, y_treinamento=None, \n",
        "                             top_n=10, figsize=(8,8), print_table=False, title=\"Feature Importances\"):\n",
        "    '''\n",
        "    plot feature importances of a tree-based sklearn estimator\n",
        "    \n",
        "    Note: X_treinamento and y_treinamento are pandas DataFrames\n",
        "    \n",
        "    Note: Scikit-plot is a lovely package but I sometimes have issues\n",
        "              1. flexibility/extendibility\n",
        "              2. complicated models/datasets\n",
        "          But for many situations Scikit-plot is the way to go\n",
        "          see https://scikit-plot.readthedocs.io/en/latest/Quickstart.html\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "        clf         (sklearn estimator) if not fitted, this routine will fit it\n",
        "        \n",
        "        X_treinamento     (pandas DataFrame)\n",
        "        \n",
        "        y_treinamento     (pandas DataFrame)  optional\n",
        "                                        required only if clf has not already been fitted \n",
        "        \n",
        "        top_n       (int)               Plot the top_n most-important features\n",
        "                                        Default: 10\n",
        "                                        \n",
        "        figsize     ((int,int))         The physical size of the plot\n",
        "                                        Default: (8,8)\n",
        "        \n",
        "        print_table (boolean)           If True, print out the table of feature importances\n",
        "                                        Default: False\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "        the pandas dataframe with the features and their importance\n",
        "        \n",
        "    Author\n",
        "    ------\n",
        "        George Fisher\n",
        "    '''\n",
        "    \n",
        "    __name__ = \"mostra_feature_importances\"\n",
        "    \n",
        "    import pandas as pd\n",
        "    import numpy  as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from xgboost.core     import XGBoostError\n",
        "    from lightgbm.sklearn import LightGBMError\n",
        "    \n",
        "    try: \n",
        "        if not hasattr(clf, 'feature_importances_'):\n",
        "            clf.fit(X_treinamento.values, y_treinamento.values.ravel())\n",
        "\n",
        "            if not hasattr(clf, 'feature_importances_'):\n",
        "                raise AttributeError(\"{} does not have feature_importances_ attribute\".\n",
        "                                    format(clf.__class__.__name__))\n",
        "                \n",
        "    except (XGBoostError, LightGBMError, ValueError):\n",
        "        clf.fit(X_treinamento.values, y_treinamento.values.ravel())\n",
        "            \n",
        "    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n",
        "    feat_imp['feature'] = X_treinamento.columns\n",
        "    feat_imp.sort_values(by ='importance', ascending = False, inplace = True)\n",
        "    feat_imp = feat_imp.iloc[:top_n]\n",
        "    \n",
        "    feat_imp.sort_values(by='importance', inplace = True)\n",
        "    feat_imp = feat_imp.set_index('feature', drop = True)\n",
        "    feat_imp.plot.barh(title=title, figsize=figsize)\n",
        "    plt.xlabel('Feature Importance Score')\n",
        "    plt.show()\n",
        "    \n",
        "    if print_table:\n",
        "        from IPython.display import display\n",
        "        print(\"Top {} features in descending order of importance\".format(top_n))\n",
        "        display(feat_imp.sort_values(by = 'importance', ascending = False))\n",
        "        \n",
        "    return feat_imp"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsH9dMxazWCg"
      },
      "source": [
        "# **DATAFRAME-EXEMPLO USADO NESTE TUTORIAL**\n",
        "> Gerar um dataframe com 18 colunas, sendo 9 informativas, 6 redundantes e 3 repetidas:\n",
        "\n",
        "Para saber mais sobre a geração de dataframes-exemplo (toy), consulte [Synthetic data generation — a must-have skill for new data scientists](https://towardsdatascience.com/synthetic-data-generation-a-must-have-skill-for-new-data-scientists-915896c0c1ae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEyDo_EIV_jV"
      },
      "source": [
        "## Definir variáveis globais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdwgpZ76WFaT"
      },
      "source": [
        "i_CV = 10 # Número de Cross-Validations\n",
        "i_Seed = 20111974 # semente por questões de reproducibilidade\n",
        "f_Test_Size = 0.3 # Proporção do dataframe de validação (outros valores poderiam ser 0.15, 0.20 ou 0.25)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJTJfpwWzykS"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(n_samples = 1000, \n",
        "                           n_features = 18, \n",
        "                           n_informative = 9, \n",
        "                           n_redundant = 6, \n",
        "                           n_repeated = 3, \n",
        "                           n_classes = 2, \n",
        "                           n_clusters_per_class = 1, \n",
        "                           random_state=i_Seed)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWy2IZh3s-o3",
        "outputId": "6220b9f4-8b86-41f1-da8f-64f89edcbf56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.06844089,  4.21184154, -2.5583024 , ..., -0.63061895,\n",
              "        -0.97831983, -0.88826977],\n",
              "       [-4.8240213 ,  0.17950903, -2.98447332, ...,  0.33992045,\n",
              "         1.89153784, -6.10967565],\n",
              "       [ 1.38953042, -0.226476  ,  1.8774004 , ..., -1.47784549,\n",
              "         0.96052606,  2.06020368],\n",
              "       ...,\n",
              "       [ 1.62548685,  0.43377848,  4.93537285, ..., -4.61990917,\n",
              "         0.18310709,  6.16040231],\n",
              "       [-2.40619087, -1.65474635,  2.64196493, ..., -1.21427845,\n",
              "         0.83745861,  0.8254619 ],\n",
              "       [-4.00041881,  2.52475556, -4.15290177, ..., -0.51680266,\n",
              "         1.72224835, -5.59558306]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccjhGnzxtAaV",
        "outputId": "285aff38-6f23-4e83-f83f-0227c0f728f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y[0:30] # Semelhante aos casos de fraude: {0, 1}"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 1, 0, 1, 0, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHO2befKJxR3"
      },
      "source": [
        "___\n",
        "# **DECISION TREE**\n",
        "> Decision Trees possuem estrutura em forma de árvores.\n",
        "\n",
        "* **Principais Vantagens**:\n",
        "    * São algoritmos fáceis de entender, visualizar e interpretar;\n",
        "    * Captura facilmente padrões não-lineares presentes nos dados;\n",
        "    * Requer pouco poder computacional --> Treinar Decision Trees não requer tanto recurso computacional!\n",
        "    * Lida bem com COLUNAS numéricas ou categóricas;\n",
        "    * Não requer os dados sejam normalizados;\n",
        "    * Pode ser utilizado como Feature Engineering ao lidar com Missing Values;\n",
        "    * Pode ser utilizado como Feature Selection;\n",
        "    * Não requer suposições sobre a distribuição dos dados por causa da natureza não-paramétrica do algoritmo\n",
        "\n",
        "* **Principais desvantagens**\n",
        "    * Propenso a Overfitting, pois Decision Trees podem construir árvores complexas que não sejam capazes de generalizar bem os dados. As coisas complicam muito se a amostra de treinamento possuir outliers. Portanto, **recomenda-se fortemente a tratar os outliers previamente**.\n",
        "    * Pode criar árvores viesadas se tivermos um dataframe não-balanceado ou que alguma classe seja dominante. Por conta disso, **recomenda-se balancear o dataframe previamente para se evitar esse problema**.\n",
        "\n",
        "* **Principais parâmetros**\n",
        "    * **Gini Index** - é uma métrica que mede a frequência com que um ponto/observação aleatoriamente selecionado seria incorretamente identificado.\n",
        "        * Portanto, quanto menor o valor de Gini Index, melhor a COLUNA;\n",
        "    * **Entropy** - é uma métrica que mede aleatoriedade da informação presente nos dados.\n",
        "        * Portanto, quanto maior a entropia da COLUNA, pior ela se torna para nos ajudar a tomar uma conclusão (classificar, por exemplo).\n",
        "\n",
        "## **Referências**:\n",
        "* [1.10. Decision Trees](https://scikit-learn.org/stable/modules/tree.html).\n",
        "* [Decision Tree Algorithm With Hands On Example](https://medium.com/datadriveninvestor/decision-tree-algorithm-with-hands-on-example-e6c2afb40d38) - ótimo tutorial para aprender, entender, interpretar e calcular os índices de Gini e entropia.\n",
        "* [Intuitive Guide to Understanding Decision Trees](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-decision-trees-adb2165ccab7) - ótimo tutorial para aprender, entender, interpretar e calcular os índices de Gini e entropia.\n",
        "* [The Complete Guide to Decision Trees](https://towardsdatascience.com/the-complete-guide-to-decision-trees-28a4e3c7be14)\n",
        "* [Creating and Visualizing Decision Tree Algorithm in Machine Learning Using Sklearn](https://intellipaat.com/blog/decision-tree-algorithm-in-machine-learning/) - Muito didático!\n",
        "* [Decision Trees in Machine Learning](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrMkPN5aLp0Y"
      },
      "source": [
        "## Carregar as bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVU1CM0PKgO4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15clh4XrISpz"
      },
      "source": [
        "## Carregar/Ler os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMPL46w2IWJw"
      },
      "source": [
        "l_colunas = ['v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18']\n",
        "\n",
        "df_X = pd.DataFrame(X, columns = l_colunas)\n",
        "df_y = pd.DataFrame(y, columns = ['target'])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFaQF2MGFl_M",
        "outputId": "4e2b2297-cc86-4e6a-f7b8-ce29772141ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "df_X.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         v1        v2        v3        v4        v5        v6        v7  \\\n",
              "0  0.068441  4.211842 -2.558302  3.665482 -3.835158  3.499851  2.490856   \n",
              "1 -4.824021  0.179509 -2.984473  1.033618 -3.893426  3.428734 -3.334605   \n",
              "2  1.389530 -0.226476  1.877400  2.713426  4.630257  0.516455 -3.743027   \n",
              "3  1.145809  2.255946  0.207364  4.665817  2.294678  6.501306  0.964770   \n",
              "4 -0.936646  3.697163 -3.363617  3.805126 -1.754430  4.954346  0.406605   \n",
              "\n",
              "         v8        v9       v10       v11       v12       v13       v14  \\\n",
              "0  3.665482  0.245117  0.867172  2.865546  0.493956 -5.148596  2.865546   \n",
              "1  1.033618 -0.882780 -0.753281  1.441522 -1.395514 -4.002880  1.441522   \n",
              "2  2.713426  1.284039  2.030797 -1.095536  1.560159 -1.014211 -1.095536   \n",
              "3  4.665817  0.119410  3.196354  1.894787  3.519138 -4.757807  1.894787   \n",
              "4  3.805126 -0.824738  1.382591  1.665704 -0.649758 -3.513036  1.665704   \n",
              "\n",
              "        v15       v16       v17       v18  \n",
              "0  3.499851 -0.630619 -0.978320 -0.888270  \n",
              "1  3.428734  0.339920  1.891538 -6.109676  \n",
              "2  0.516455 -1.477845  0.960526  2.060204  \n",
              "3  6.501306 -3.789029  0.579491  1.397106  \n",
              "4  4.954346  0.257052  0.904244 -3.071354  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a1114340-bdfe-458a-b957-921db47f8e87\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>v3</th>\n",
              "      <th>v4</th>\n",
              "      <th>v5</th>\n",
              "      <th>v6</th>\n",
              "      <th>v7</th>\n",
              "      <th>v8</th>\n",
              "      <th>v9</th>\n",
              "      <th>v10</th>\n",
              "      <th>v11</th>\n",
              "      <th>v12</th>\n",
              "      <th>v13</th>\n",
              "      <th>v14</th>\n",
              "      <th>v15</th>\n",
              "      <th>v16</th>\n",
              "      <th>v17</th>\n",
              "      <th>v18</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.068441</td>\n",
              "      <td>4.211842</td>\n",
              "      <td>-2.558302</td>\n",
              "      <td>3.665482</td>\n",
              "      <td>-3.835158</td>\n",
              "      <td>3.499851</td>\n",
              "      <td>2.490856</td>\n",
              "      <td>3.665482</td>\n",
              "      <td>0.245117</td>\n",
              "      <td>0.867172</td>\n",
              "      <td>2.865546</td>\n",
              "      <td>0.493956</td>\n",
              "      <td>-5.148596</td>\n",
              "      <td>2.865546</td>\n",
              "      <td>3.499851</td>\n",
              "      <td>-0.630619</td>\n",
              "      <td>-0.978320</td>\n",
              "      <td>-0.888270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-4.824021</td>\n",
              "      <td>0.179509</td>\n",
              "      <td>-2.984473</td>\n",
              "      <td>1.033618</td>\n",
              "      <td>-3.893426</td>\n",
              "      <td>3.428734</td>\n",
              "      <td>-3.334605</td>\n",
              "      <td>1.033618</td>\n",
              "      <td>-0.882780</td>\n",
              "      <td>-0.753281</td>\n",
              "      <td>1.441522</td>\n",
              "      <td>-1.395514</td>\n",
              "      <td>-4.002880</td>\n",
              "      <td>1.441522</td>\n",
              "      <td>3.428734</td>\n",
              "      <td>0.339920</td>\n",
              "      <td>1.891538</td>\n",
              "      <td>-6.109676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.389530</td>\n",
              "      <td>-0.226476</td>\n",
              "      <td>1.877400</td>\n",
              "      <td>2.713426</td>\n",
              "      <td>4.630257</td>\n",
              "      <td>0.516455</td>\n",
              "      <td>-3.743027</td>\n",
              "      <td>2.713426</td>\n",
              "      <td>1.284039</td>\n",
              "      <td>2.030797</td>\n",
              "      <td>-1.095536</td>\n",
              "      <td>1.560159</td>\n",
              "      <td>-1.014211</td>\n",
              "      <td>-1.095536</td>\n",
              "      <td>0.516455</td>\n",
              "      <td>-1.477845</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>2.060204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.145809</td>\n",
              "      <td>2.255946</td>\n",
              "      <td>0.207364</td>\n",
              "      <td>4.665817</td>\n",
              "      <td>2.294678</td>\n",
              "      <td>6.501306</td>\n",
              "      <td>0.964770</td>\n",
              "      <td>4.665817</td>\n",
              "      <td>0.119410</td>\n",
              "      <td>3.196354</td>\n",
              "      <td>1.894787</td>\n",
              "      <td>3.519138</td>\n",
              "      <td>-4.757807</td>\n",
              "      <td>1.894787</td>\n",
              "      <td>6.501306</td>\n",
              "      <td>-3.789029</td>\n",
              "      <td>0.579491</td>\n",
              "      <td>1.397106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.936646</td>\n",
              "      <td>3.697163</td>\n",
              "      <td>-3.363617</td>\n",
              "      <td>3.805126</td>\n",
              "      <td>-1.754430</td>\n",
              "      <td>4.954346</td>\n",
              "      <td>0.406605</td>\n",
              "      <td>3.805126</td>\n",
              "      <td>-0.824738</td>\n",
              "      <td>1.382591</td>\n",
              "      <td>1.665704</td>\n",
              "      <td>-0.649758</td>\n",
              "      <td>-3.513036</td>\n",
              "      <td>1.665704</td>\n",
              "      <td>4.954346</td>\n",
              "      <td>0.257052</td>\n",
              "      <td>0.904244</td>\n",
              "      <td>-3.071354</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1114340-bdfe-458a-b957-921db47f8e87')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a1114340-bdfe-458a-b957-921db47f8e87 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a1114340-bdfe-458a-b957-921db47f8e87');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-ibdD2ZG7tm",
        "outputId": "f2cc897d-2398-46e7-c9a2-2098adda235f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_X.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9cqRaywa_TR",
        "outputId": "d446e0b6-ebc7-4074-f96e-656990f94506",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "set(df_y['target'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN6jbpn6Iwmu"
      },
      "source": [
        "## Estatísticas Descritivas básicas do dataframe - df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlwhxxUNIyYs",
        "outputId": "9f1f16a3-5976-4e8e-cf5d-1b49d5716821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        }
      },
      "source": [
        "df_X.describe()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                v1           v2           v3           v4           v5  \\\n",
              "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
              "mean     -0.085159     1.034227     0.657408     1.405317     0.687279   \n",
              "std       2.002247     1.631507     3.608772     2.256857     4.019598   \n",
              "min      -6.944169    -4.620754   -16.300139    -6.235192   -12.454256   \n",
              "25%      -1.305566    -0.089052    -1.623657    -0.152888    -1.854645   \n",
              "50%       0.052523     0.994150     0.573849     1.449931     0.812364   \n",
              "75%       1.383853     2.071995     3.038586     2.887141     3.413952   \n",
              "max       4.997172     7.354860    11.720165     8.494566    12.844418   \n",
              "\n",
              "                v6           v7           v8           v9          v10  \\\n",
              "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
              "mean      1.131560     0.108053     1.405317     1.007023     1.048801   \n",
              "std       4.481832     1.981307     2.256857     1.863288     1.643900   \n",
              "min     -14.305401    -6.152747    -6.235192    -5.484992    -3.293216   \n",
              "25%      -1.684751    -1.216983    -0.152888    -0.240908    -0.012710   \n",
              "50%       1.281504     0.167091     1.449931     1.066125     1.012899   \n",
              "75%       4.008103     1.438719     2.887141     2.288188     2.187202   \n",
              "max      15.999803     6.293550     8.494566     8.146559     6.523180   \n",
              "\n",
              "               v11          v12          v13          v14          v15  \\\n",
              "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
              "mean      0.079248     0.001650    -0.365438     0.079248     1.131560   \n",
              "std       1.949273     1.932641     4.160668     1.949273     4.481832   \n",
              "min      -7.135349    -5.705500    -9.120941    -7.135349   -14.305401   \n",
              "25%      -1.209675    -1.292162    -3.555363    -1.209675    -1.684751   \n",
              "50%       0.180344     0.035237    -0.966638     0.180344     1.281504   \n",
              "75%       1.439199     1.315342     2.745806     1.439199     4.008103   \n",
              "max       6.252448     5.538216    11.259350     6.252448    15.999803   \n",
              "\n",
              "               v16          v17          v18  \n",
              "count  1000.000000  1000.000000  1000.000000  \n",
              "mean     -0.027751     0.984606     0.633624  \n",
              "std       2.065455     1.850593     3.552991  \n",
              "min      -6.009023    -5.035184   -11.439074  \n",
              "25%      -1.436673    -0.261610    -1.691346  \n",
              "50%      -0.000190     0.975793     0.844784  \n",
              "75%       1.365369     2.256504     3.109330  \n",
              "max       6.531561     7.646802    12.090528  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c6256187-061d-4d54-b1d6-5e8fee3f1862\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>v3</th>\n",
              "      <th>v4</th>\n",
              "      <th>v5</th>\n",
              "      <th>v6</th>\n",
              "      <th>v7</th>\n",
              "      <th>v8</th>\n",
              "      <th>v9</th>\n",
              "      <th>v10</th>\n",
              "      <th>v11</th>\n",
              "      <th>v12</th>\n",
              "      <th>v13</th>\n",
              "      <th>v14</th>\n",
              "      <th>v15</th>\n",
              "      <th>v16</th>\n",
              "      <th>v17</th>\n",
              "      <th>v18</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-0.085159</td>\n",
              "      <td>1.034227</td>\n",
              "      <td>0.657408</td>\n",
              "      <td>1.405317</td>\n",
              "      <td>0.687279</td>\n",
              "      <td>1.131560</td>\n",
              "      <td>0.108053</td>\n",
              "      <td>1.405317</td>\n",
              "      <td>1.007023</td>\n",
              "      <td>1.048801</td>\n",
              "      <td>0.079248</td>\n",
              "      <td>0.001650</td>\n",
              "      <td>-0.365438</td>\n",
              "      <td>0.079248</td>\n",
              "      <td>1.131560</td>\n",
              "      <td>-0.027751</td>\n",
              "      <td>0.984606</td>\n",
              "      <td>0.633624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.002247</td>\n",
              "      <td>1.631507</td>\n",
              "      <td>3.608772</td>\n",
              "      <td>2.256857</td>\n",
              "      <td>4.019598</td>\n",
              "      <td>4.481832</td>\n",
              "      <td>1.981307</td>\n",
              "      <td>2.256857</td>\n",
              "      <td>1.863288</td>\n",
              "      <td>1.643900</td>\n",
              "      <td>1.949273</td>\n",
              "      <td>1.932641</td>\n",
              "      <td>4.160668</td>\n",
              "      <td>1.949273</td>\n",
              "      <td>4.481832</td>\n",
              "      <td>2.065455</td>\n",
              "      <td>1.850593</td>\n",
              "      <td>3.552991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-6.944169</td>\n",
              "      <td>-4.620754</td>\n",
              "      <td>-16.300139</td>\n",
              "      <td>-6.235192</td>\n",
              "      <td>-12.454256</td>\n",
              "      <td>-14.305401</td>\n",
              "      <td>-6.152747</td>\n",
              "      <td>-6.235192</td>\n",
              "      <td>-5.484992</td>\n",
              "      <td>-3.293216</td>\n",
              "      <td>-7.135349</td>\n",
              "      <td>-5.705500</td>\n",
              "      <td>-9.120941</td>\n",
              "      <td>-7.135349</td>\n",
              "      <td>-14.305401</td>\n",
              "      <td>-6.009023</td>\n",
              "      <td>-5.035184</td>\n",
              "      <td>-11.439074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-1.305566</td>\n",
              "      <td>-0.089052</td>\n",
              "      <td>-1.623657</td>\n",
              "      <td>-0.152888</td>\n",
              "      <td>-1.854645</td>\n",
              "      <td>-1.684751</td>\n",
              "      <td>-1.216983</td>\n",
              "      <td>-0.152888</td>\n",
              "      <td>-0.240908</td>\n",
              "      <td>-0.012710</td>\n",
              "      <td>-1.209675</td>\n",
              "      <td>-1.292162</td>\n",
              "      <td>-3.555363</td>\n",
              "      <td>-1.209675</td>\n",
              "      <td>-1.684751</td>\n",
              "      <td>-1.436673</td>\n",
              "      <td>-0.261610</td>\n",
              "      <td>-1.691346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.052523</td>\n",
              "      <td>0.994150</td>\n",
              "      <td>0.573849</td>\n",
              "      <td>1.449931</td>\n",
              "      <td>0.812364</td>\n",
              "      <td>1.281504</td>\n",
              "      <td>0.167091</td>\n",
              "      <td>1.449931</td>\n",
              "      <td>1.066125</td>\n",
              "      <td>1.012899</td>\n",
              "      <td>0.180344</td>\n",
              "      <td>0.035237</td>\n",
              "      <td>-0.966638</td>\n",
              "      <td>0.180344</td>\n",
              "      <td>1.281504</td>\n",
              "      <td>-0.000190</td>\n",
              "      <td>0.975793</td>\n",
              "      <td>0.844784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.383853</td>\n",
              "      <td>2.071995</td>\n",
              "      <td>3.038586</td>\n",
              "      <td>2.887141</td>\n",
              "      <td>3.413952</td>\n",
              "      <td>4.008103</td>\n",
              "      <td>1.438719</td>\n",
              "      <td>2.887141</td>\n",
              "      <td>2.288188</td>\n",
              "      <td>2.187202</td>\n",
              "      <td>1.439199</td>\n",
              "      <td>1.315342</td>\n",
              "      <td>2.745806</td>\n",
              "      <td>1.439199</td>\n",
              "      <td>4.008103</td>\n",
              "      <td>1.365369</td>\n",
              "      <td>2.256504</td>\n",
              "      <td>3.109330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.997172</td>\n",
              "      <td>7.354860</td>\n",
              "      <td>11.720165</td>\n",
              "      <td>8.494566</td>\n",
              "      <td>12.844418</td>\n",
              "      <td>15.999803</td>\n",
              "      <td>6.293550</td>\n",
              "      <td>8.494566</td>\n",
              "      <td>8.146559</td>\n",
              "      <td>6.523180</td>\n",
              "      <td>6.252448</td>\n",
              "      <td>5.538216</td>\n",
              "      <td>11.259350</td>\n",
              "      <td>6.252448</td>\n",
              "      <td>15.999803</td>\n",
              "      <td>6.531561</td>\n",
              "      <td>7.646802</td>\n",
              "      <td>12.090528</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6256187-061d-4d54-b1d6-5e8fee3f1862')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c6256187-061d-4d54-b1d6-5e8fee3f1862 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c6256187-061d-4d54-b1d6-5e8fee3f1862');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_QhFqyZOKFB"
      },
      "source": [
        "## Selecionar as amostras de treinamento e validação\n",
        "\n",
        "* Dividir os dados/amostras em:\n",
        "    * **Amostra de treinamento**: usado para treinar o modelo e otimizar os hiperparâmetros;\n",
        "    * **Amostra de teste**: usado para verificar se o modelo otimizado funciona em dados totalmente desconhecidos. É nesta amostra de teste que avaliamos a performance do modelo em termos de generalização (trabalhar com dados que não lhe foi apresentado);\n",
        "* **Técnica de Hold-Out**: Separar/dividir os dados em amostra de treinamento e teste. Geralmente usamos 70% da amostra para treinamento e 30% validação. Outras opções são usar os percentuais 80/20 ou 75/25 (default).\n",
        "    * **Desvatangem do Hold-Out**: Variância nos dados pode comprometer performance do modelo quando, por exemplo, amostra de treinamento é semelhante amostra de teste. \n",
        "* Consulte [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) para mais detalhes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sKBgs-QOOfn"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(df_X, df_y, test_size = f_Test_Size, random_state = i_Seed)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPTKBBHgOpoA",
        "outputId": "07a31925-f363-4126-babd-fa5f483a992a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_treinamento.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(700, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEn_LLs2OtRI",
        "outputId": "e8cd2699-400a-48cb-9aac-8e628ce838fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_treinamento.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(700, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uAw8EcyOvrG",
        "outputId": "f8bc7e13-abec-4820-b784-0baf73d31d52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_teste.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2LYI-9hOyXI",
        "outputId": "7c29bc6e-184b-428d-847a-8e618daf471e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_teste.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npgoBSX2dd4l"
      },
      "source": [
        "## Treinar o algoritmo com os dados de treinamento\n",
        "### Carregar os algoritmos/libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcvzrtolGfnQ",
        "outputId": "a1e8badd-8ed2-47ec-afb4-058c799bd5fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install graphviz\n",
        "!pip install pydotplus\n",
        "\n",
        "import six\n",
        "import sys\n",
        "sys.modules['sklearn.externals.six'] = six"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydotplus in /usr/local/lib/python3.7/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from pydotplus) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_pF-HH3JKL2"
      },
      "source": [
        "from sklearn.metrics import accuracy_score # para medir a acurácia do modelo preditivo\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix # para plotar a confusion matrix\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV # para otimizar os parâmetros dos modelos preditivos\n",
        "from sklearn.model_selection import cross_val_score # Para o CV (Cross-Validation)\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "from time import time\n",
        "from operator import itemgetter\n",
        "from scipy.stats import randint\n",
        "\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.externals.six import StringIO  \n",
        "from IPython.display import Image  \n",
        "import pydotplus\n",
        "\n",
        "np.set_printoptions(suppress=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJMS9ePQ6B6t"
      },
      "source": [
        "**Atenção**: Para evitar overfitting nos algoritmos DecisionTreeClassifier, considere min_samples_split = 2 como default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNeRHYePJc-r",
        "outputId": "2be267e9-1cba-4a6c-c8e2-1b99244a4ac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier # Library para Decision Tree (Classificação)\n",
        "\n",
        "# Instancia (configuração do Decision Trees) com os parâmetros sugeridos para se evitar overfitting:\n",
        "ml_DT = DecisionTreeClassifier(criterion = 'gini', \n",
        "                               splitter = 'best', \n",
        "                               max_depth = None, \n",
        "                               min_samples_split = 2, \n",
        "                               min_samples_leaf = 1, \n",
        "                               min_weight_fraction_leaf = 0.0, \n",
        "                               max_features = None, \n",
        "                               random_state = i_Seed, \n",
        "                               max_leaf_nodes = None, \n",
        "                               #min_impurity_decrease = 0.0, \n",
        "                               min_impurity_split = None, \n",
        "                               class_weight = None, \n",
        "                               #presort = False\n",
        "                               )"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6ae106b35285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                                \u001b[0mmin_impurity_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                presort = False)\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'min_impurity_split'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVLZznprx2YX"
      },
      "source": [
        "# Objeto/classificador configurado\n",
        "ml_DT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CC24H-JHhlj"
      },
      "source": [
        "### Treina o algoritmo: fit(df)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgAHfXVo-Nw8"
      },
      "source": [
        "ml_DT.fit(X_treinamento, y_treinamento)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNiRjmrRHVnx"
      },
      "source": [
        "### Valida o modelo com a amostra de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GMCSs89HquJ"
      },
      "source": [
        "ml_DT.score(X_teste, y_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmv9YZobIer4"
      },
      "source": [
        "### Calcula as predições usando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YufZaRNIkFL"
      },
      "source": [
        "y_pred = ml_DT.predict(X_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYvMN-tvIX-p"
      },
      "source": [
        "### Matriz de Confusão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iTK6pBwIZ3F"
      },
      "source": [
        "cf_matrix = confusion_matrix(y_teste, y_pred)\n",
        "cf_labels = ['True_Negative', 'False_Positive', 'False_Negative', 'True_Positive']\n",
        "cf_categories = ['Zero', 'One']\n",
        "mostra_confusion_matrix(cf_matrix, group_names= cf_labels, categories= cf_categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOnkFBcEIVAb"
      },
      "source": [
        "### Volte ao início, extraia nova amostra e calcule a acurácia\n",
        "* Observou que a acurácia mudou? Isso acontece porque extraimos uma nova amostra de treinamento.\n",
        "* Quais os inconvenientes de termos uma métrica diferente para cada amostra do modelo preditivo?\n",
        "* Como reportar os resultados do seu modelo?\n",
        "* Como se assegurar acerca do valor mais ideal da métrica?\n",
        "    * use a Estatística a seu favor! --> Use Cross-Validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkBSvyorGXQz"
      },
      "source": [
        "___\n",
        "# **CROSS-VALIDATION**\n",
        "* K-fold é o método de Cross-Validation (CV) mais conhecido e utilizado;\n",
        "* Como funciona: divide o dataframe de treinamento em k partes (cada parte é um fold);\n",
        "    * Usa k-1 partes para treinar o modelo e o restante para validar o modelo;\n",
        "    * O processo é repetido k vezes, sendo que em cada iteração calcula as métricas desejadas (exemplo: acurácia);\n",
        "    * Desta forma o modelo é treinado e testado com todas as partes dos dados;\n",
        "    * Ao final das k iterações, teremos k métricas das quais calculamos média e desvio-padrão.\n",
        "\n",
        " A figura abaixo nos ajuda a entender como funciona CV:\n",
        "\n",
        "![Cross-Validation](https://github.com/MathMachado/Materials/blob/master/CV2.PNG?raw=true)\n",
        "\n",
        "Source: [5 Reasons why you should use Cross-Validation in your Data Science Projects](https://towardsdatascience.com/5-reasons-why-you-should-use-cross-validation-in-your-data-science-project-8163311a1e79)\n",
        "\n",
        "* **valor de k**:\n",
        "    * valor de k (folds): entre 5 e 10 --> Não há regra geral para a escolha de k;\n",
        "    * Quanto maior o valor de k --> menor o viés do CV --> Experimento Estatístico para mostrar o efeito.\n",
        "\n",
        "[Applied Predictive Modeling, 2013](https://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/ref=as_li_ss_tl?ie=UTF8&qid=1520380699&sr=8-1&keywords=applied+predictive+modeling&linkCode=sl1&tag=inspiredalgor-20&linkId=1af1f3de89c11e4a7fd49de2b05e5ebf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HscfN-a1V043"
      },
      "source": [
        "* **Vantagens do uso de CV**:\n",
        "    * Modelos com melhor acurácia;\n",
        "    * Melhor uso dos dados, pois todos os dados são utilizados como treinamento e validação. Portanto, qualquer problema com os dados serão encontrados nesta fase.\n",
        "\n",
        "* **Leitura Adicional**\n",
        "    * [Cross-Validation in Machine Learning](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f)\n",
        "    * [5 Reasons why you should use Cross-Validation in your Data Science Projects](https://towardsdatascience.com/5-reasons-why-you-should-use-cross-validation-in-your-data-science-project-8163311a1e79)\n",
        "    * [Cross-validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x2UPwOYQPcI"
      },
      "source": [
        "# Cross-Validation com k = 10 folds (= 10 partes)\n",
        "a_scores_CV = funcao_cross_val_score(ml_DT, X_treinamento, y_treinamento, i_CV)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uxoplcea0byV"
      },
      "source": [
        "a_scores_CV # array com os scores a cada iteração do CV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3k-PcbN0o_i"
      },
      "source": [
        "a_scores_CV.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_rYker2gzeG"
      },
      "source": [
        "**Interpretação**: Nosso classificador (DecisionTreeClassifier) tem uma acurácia média de 91,43% (base de treinamento). Além disso, o std é da ordem de 3,66%, ou seja, pequena. Vamos tentar melhorar a acurácia do classificador usando parameter tunning (GridSearchCV)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkwchmkP3p_A"
      },
      "source": [
        "print(f'Acurácias: {a_scores_CV}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQNyqHCiKRUh"
      },
      "source": [
        "### Valida o modelo com a amostra de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb0ZPyvKKRUp"
      },
      "source": [
        "ml_DT.score(X_teste, y_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL2tEdbqKY5P"
      },
      "source": [
        "### Predições com o modelo treinado\n",
        "* Faz predições usando o classificador (Decision Trees) para inferir na amostra de teste:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI31WkZs2ht_"
      },
      "source": [
        "y_pred = ml_DT.predict(X_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfapj3OG13PG"
      },
      "source": [
        "y_pred[0:30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc88ofqh16RT"
      },
      "source": [
        "y[0:30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cecv-51TKgz-"
      },
      "source": [
        "### Matriz de Confusão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSaVzJ9xFpwW"
      },
      "source": [
        "cf_matrix = confusion_matrix(y_teste, y_pred)\n",
        "cf_labels = ['True_Negative', 'False_Positive', 'False_Negative', 'True_Positive']\n",
        "cf_categories = ['Zero', 'One']\n",
        "mostra_confusion_matrix(cf_matrix, group_names= cf_labels, categories= cf_categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8D975NqsGtj"
      },
      "source": [
        "## Parameter tunning\n",
        "### Referência\n",
        "* [Hyperparameter Tuning the Random Forest in Python](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)\n",
        "* [Decision Tree Adventures 2 — Explanation of Decision Tree Classifier Parameters](https://medium.com/datadriveninvestor/decision-tree-adventures-2-explanation-of-decision-tree-classifier-parameters-84776f39a28) - Explica didaticamente e step by step como fazer parameter tunning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfdq5zEhlVsk"
      },
      "source": [
        "# Dicionário com hiperparâmetros para o parameter tunning. Ao todo serão ajustados 2X13X5X5X7= 4.550 modelos. Contando com 10 folds no Cross-Validation, então são 45.500 modelos.\n",
        "d_hiperparametros_DT = {\"criterion\": [\"gini\", \"entropy\"], \n",
        "                   \"min_samples_split\": [2, 5, 10, 30, 50, 70, 90, 120, 150, 180, 210, 240, 270, 350, 400], \n",
        "                   \"max_depth\": [None, 2, 5, 9, 15], \n",
        "                   \"min_samples_leaf\": [20, 40, 60, 80, 100], \n",
        "                   \"max_leaf_nodes\": [None, 2, 3, 4, 5, 10, 15]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtajXuuUpGwq"
      },
      "source": [
        "d_hiperparametros_DT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8gNSs0G0A-L"
      },
      "source": [
        "```\n",
        "grid_search = GridSearchCV(ml_DT, param_grid= d_hiperparametros_DT, cv = i_CV, n_jobs= -1)\n",
        "start = time()\n",
        "grid_search.fit(X_treinamento, y_treinamento)\n",
        "tempo_elapsed= time()-start\n",
        "print(f\"\\nGridSearchCV levou {tempo_elapsed:.2f} segundos para estimar {len(grid_search.cv_results_)} modelos candidatos\")\n",
        "\n",
        "GridSearchCV levou 1999.12 segundos para estimar 23 modelos candidatos\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44-BRnNjBT25"
      },
      "source": [
        "# Invoca a função com o modelo baseline\n",
        "ml_DT2, best_params = GridSearchOptimizer(ml_DT, 'ml_DT2', d_hiperparametros_DT, X_treinamento, y_treinamento, X_teste, y_teste, i_CV, l_colunas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmCkjGjPJMLr"
      },
      "source": [
        "### Visualizar o resultado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIc3ZgaISEd0"
      },
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.externals.six import StringIO  \n",
        "from IPython.display import Image  \n",
        "import pydotplus\n",
        "\n",
        "dot_data = StringIO()\n",
        "export_graphviz(ml_DT2, out_file = dot_data, filled = True, rounded = True, special_characters = True, feature_names = l_colunas, class_names = ['0','1'])\n",
        "\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "graph.write_png('DecisionTree.png')\n",
        "Image(graph.create_png())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1R2GBkbnV37"
      },
      "source": [
        "## Selecionar as COLUNAS importantes/relevantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukMLoEr7nbUf"
      },
      "source": [
        "X_treinamento_DT, X_teste_DT = seleciona_colunas_relevantes(ml_DT2, X_treinamento, X_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JjePRQAoqkk"
      },
      "source": [
        "## Treina o classificador com as COLUNAS relevantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt3aCPpfKRxm"
      },
      "source": [
        "best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq6uCVtzovMt"
      },
      "source": [
        "# Treina usando as COLUNAS relevantes...\n",
        "ml_DT2.fit(X_treinamento_DT, y_treinamento)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2h3EpinRD5Q"
      },
      "source": [
        "# Cross-Validation com 10 folds\n",
        "a_scores_CV = funcao_cross_val_score(ml_DT2, X_treinamento, y_treinamento, i_CV)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znWy3LE1q-Z3"
      },
      "source": [
        "ml_DT3, best_params2 = GridSearchOptimizer(ml_DT2, 'ml_DT2', d_hiperparametros_DT, X_treinamento_DT, y_treinamento, X_teste_DT, y_teste, i_CV, l_colunas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IhCC6pfq-jL"
      },
      "source": [
        "best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw6Dk3kesT0q"
      },
      "source": [
        "best_params2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFoK1ZGrRHf3"
      },
      "source": [
        "# Cross-Validation com 10 folds\n",
        "a_scores_CV = funcao_cross_val_score(ml_DT3, X_treinamento_DT, y_treinamento, i_CV)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ1-vGRcxJoN"
      },
      "source": [
        "## Valida o modelo usando o dataframe X_teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig9GiUAEw9jr"
      },
      "source": [
        "y_pred_DT = ml_DT2.predict(X_teste_DT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UZz4UzHDqae"
      },
      "source": [
        "# Calcula acurácia\n",
        "accuracy_score(y_teste, y_pred_DT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3EUMAxxKBur"
      },
      "source": [
        "___\n",
        "# **RANDOM FOREST**\n",
        "* Decision Trees possuem estrutura em forma de árvores.\n",
        "* Random Forest pode ser utilizado tanto para classificação (RandomForestClassifier)quanto para Regressão (RandomForestRegressor).\n",
        "\n",
        "* **Vantagens**:\n",
        "    * Não requer tanto data preprocessing;\n",
        "    * Lida bem com COLUNAS categóricas e numéricas;\n",
        "    * É um Boosting Ensemble Method (pois constrói muitas árvores). Estes modelos aprendem com os próprios erros e ajustam as árvores de modo a fazer melhores classificações;\n",
        "    * Mais robusta que uma simples Decision Tree. **Porque?**\n",
        "    * Controla automaticamente overfitting (**porque?**) e frequentemente produz modelos muito robustos e de alta-performance.\n",
        "    * Pode ser utilizado como Feature Selection, pois gera a matriz de importância dos atributos (importance sample). A soma das importâncias soma 100;\n",
        "    * Assim como as Decision Trees, esses modelos capturam facilmente padrões não-lineares presentes nos dados;\n",
        "    * Não requer os dados sejam normalizados;\n",
        "    * Lida bem com Missing Values;\n",
        "    * Não requer suposições (assumptions) sobre a distribuição dos dados por causa da natureza não-paramétrica do algoritmo\n",
        "\n",
        "* **Desvantagens**\n",
        "    * **Recomenda-se balancear o dataframe previamente para se evitar esse problema**.\n",
        "\n",
        "* **Principais parâmetros**\n",
        "\n",
        "## **Referências**:\n",
        "* [Running Random Forests? Inspect the feature importances with this code](https://towardsdatascience.com/running-random-forests-inspect-the-feature-importances-with-this-code-2b00dd72b92e)\n",
        "* [Feature importances with forests of trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)\n",
        "* [Understanding Random Forests Classifiers in Python](https://www.datacamp.com/community/tutorials/random-forests-classifier-python)\n",
        "* [Understanding Random Forest](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)\n",
        "* [An Implementation and Explanation of the Random Forest in Python](https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76)\n",
        "* [Random Forest Simple Explanation](https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d)\n",
        "* [Random Forest Explained](https://www.youtube.com/watch?v=eM4uJ6XGnSM)\n",
        "* [Hyperparameter Tuning the Random Forest in Python](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74) - Explica os principais parâmetros do Random Forest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnfDw_GEKBuu"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Instancia...\n",
        "ml_RF= RandomForestClassifier(n_estimators=100, min_samples_split= 2, max_features=\"auto\", random_state= i_Seed)\n",
        "\n",
        "# Treina...\n",
        "ml_RF.fit(X_treinamento, y_treinamento)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E25BIxM0RTzs"
      },
      "source": [
        "# Cross-Validation com 10 folds\n",
        "a_scores_CV = funcao_cross_val_score(ml_RF, X_treinamento, y_treinamento, i_CV)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AouWUu8vANdb"
      },
      "source": [
        "**Interpretação**: Nosso classificador (RandomForestClassifier) tem uma acurácia média de 96,44% (base de treinamento). Além disso, o std é da ordem de 2,77%, ou seja, pequena. Vamos tentar melhorar a acurácia do classificador usando parameter tunning (GridSearchCV)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbducxlgAa85"
      },
      "source": [
        "print(f'Acurácias: {a_scores_CV}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lxx-LUw_5sd"
      },
      "source": [
        "# Faz predições...\n",
        "y_pred = ml_RF.predict(X_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQIRO_LpGAkw"
      },
      "source": [
        "# Confusion Matrix\n",
        "cf_matrix = confusion_matrix(y_teste, y_pred)\n",
        "cf_labels = ['True_Negative','False_Positive','False_Negative','True_Positive']\n",
        "cf_categories = ['Zero', 'One']\n",
        "mostra_confusion_matrix(cf_matrix, group_names= cf_labels, categories= cf_categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKLHZ5_C6FJ8"
      },
      "source": [
        "## Parameter tunning\n",
        "### Referência\n",
        "* [Hyperparameter Tuning the Random Forest in Python](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)\n",
        "* [Decision Tree Adventures 2 — Explanation of Decision Tree Classifier Parameters](https://medium.com/datadriveninvestor/decision-tree-adventures-2-explanation-of-decision-tree-classifier-parameters-84776f39a28) - Explica didaticamente e step by step como fazer parameter tunning.\n",
        "* [Optimizing Hyperparameters in Random Forest Classification](https://towardsdatascience.com/optimizing-hyperparameters-in-random-forest-classification-ec7741f9d3f6) - Outro approach para entender parameter tunning. Recomendo fortemente a leitura! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOa9naju6FKA"
      },
      "source": [
        "# Dicionário de parâmetros para o parameter tunning.\n",
        "d_hiperparametros_RF= {'bootstrap': [True, False]} #,\n",
        "#                  'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
        "#                  'max_features': ['auto', 'sqrt'],\n",
        "#                  'min_samples_leaf': [1, 2, 4],\n",
        "#                  'min_samples_split': [2, 5, 10],\n",
        "#                  'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6__f2jZaTQat"
      },
      "source": [
        "# Invoca a função\n",
        "ml_RF2, best_params = GridSearchOptimizer(ml_RF, 'ml_RF2', d_hiperparametros_RF, X_treinamento, y_treinamento, X_teste, y_teste, i_CV, l_colunas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crfn-n--KG4n"
      },
      "source": [
        "### Resultado da execução do Random Forest\n",
        "\n",
        "```\n",
        "[Parallel(n_jobs=-1)]: Done 7920 out of 7920 | elapsed: 194.0min finished\n",
        "best_params= {'bootstrap': False, 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 400}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGTOe5PaRw59"
      },
      "source": [
        "# Como o procedimento acima levou 194 minutos para executar, então vou estimar ml_RF2 abaixo usando os parâmetros acima estimados\n",
        "best_params= {'bootstrap': False, 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 400}\n",
        "\n",
        "ml_RF2= RandomForestClassifier(bootstrap= best_params['bootstrap'], \n",
        "                                  max_depth= best_params['max_depth'], \n",
        "                                  max_features= best_params['max_features'], \n",
        "                                  min_samples_leaf= best_params['min_samples_leaf'], \n",
        "                                  min_samples_split= best_params['min_samples_split'], \n",
        "                                  n_estimators= best_params['n_estimators'], \n",
        "                                  random_state= i_Seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMJcAdLlTQa0"
      },
      "source": [
        "## Visualizar o resultado\n",
        "> Implementar a visualização do RandomForest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWNiy7Z0TQa3"
      },
      "source": [
        "## Selecionar as COLUNAS importantes/relevantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOi11YOKTQa4"
      },
      "source": [
        "X_treinamento_RF, X_teste_RF = seleciona_colunas_relevantes(ml_RF2, X_treinamento, X_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn_O7c_DTQbE"
      },
      "source": [
        "## Treina o classificador com as COLUNAS relevantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwEOwzSGTQbF"
      },
      "source": [
        "best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr8qDrgvTQbL"
      },
      "source": [
        "# Treina com as COLUNAS relevantes...\n",
        "ml_RF2.fit(X_treinamento_RF, y_treinamento)\n",
        "\n",
        "# Cross-Validation com 10 folds\n",
        "a_scores_CV = funcao_cross_val_score(ml_RF2, X_treinamento_RF, y_treinamento, i_CV)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mYfQLlsTQbQ"
      },
      "source": [
        "## Valida o modelo usando o dataframe X_teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSD5o1JQTQbR"
      },
      "source": [
        "y_pred_RF = ml_RF2.predict(X_teste_RF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wywF6LymDzKr"
      },
      "source": [
        "# Calcula acurácia\n",
        "accuracy_score(y_teste, y_pred_RF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJJsL0IJb6iO"
      },
      "source": [
        "## Estudo do comportamento dos parametros do algoritmo\n",
        "> Consulte [Optimizing Hyperparameters in Random Forest Classification](https://towardsdatascience.com/optimizing-hyperparameters-in-random-forest-classification-ec7741f9d3f6) para mais detalhes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "navUWMwHi44D"
      },
      "source": [
        "param_range = np.arange(1, 250, 2)\n",
        "\n",
        "# Calculate accuracy on training and test set using range of parameter values\n",
        "train_a_scores_CV, test_a_scores_CV = validation_curve(RandomForestClassifier(), \n",
        "                                                       X_treinamento, \n",
        "                                                       y_treinamento, \n",
        "                                                       param_name=\"n_estimators\", \n",
        "                                                       param_range = param_range, \n",
        "                                                       cv = i_CV, \n",
        "                                                       scoring = \"accuracy\", \n",
        "                                                       n_jobs = -1)\n",
        "\n",
        "\n",
        "# Calculate mean and standard deviation for training set a_scores_CV\n",
        "train_mean = np.mean(train_a_scores_CV, axis = 1)\n",
        "train_std = np.std(train_a_scores_CV, axis = 1)\n",
        "\n",
        "# Calculate mean and standard deviation for test set a_scores_CV\n",
        "test_mean = np.mean(test_a_scores_CV, axis = 1)\n",
        "test_std = np.std(test_a_scores_CV, axis = 1)\n",
        "\n",
        "# Plot mean accuracy a_scores_CV for training and test sets\n",
        "plt.plot(param_range, train_mean, label = \"Training score\", color = \"black\")\n",
        "plt.plot(param_range, test_mean, label = \"Cross-validation score\", color = \"dimgrey\")\n",
        "\n",
        "# Plot accurancy bands for training and test sets\n",
        "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color = \"gray\")\n",
        "plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color = \"gainsboro\")\n",
        "\n",
        "# Create plot\n",
        "plt.title(\"Validation Curve With Random Forest\")\n",
        "plt.xlabel(\"Number Of Trees\")\n",
        "plt.ylabel(\"Accuracy Score\")\n",
        "plt.tight_layout()\n",
        "plt.legend(loc = \"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv7TIM9kjsud"
      },
      "source": [
        "param_range = np.arange(1, 250, 2)\n",
        "\n",
        "# Calculate accuracy on training and test set using range of parameter values\n",
        "train_a_scores_CV, test_a_scores_CV = validation_curve(RandomForestClassifier(), \n",
        "                                                       X_treinamento, \n",
        "                                                       y_treinamento, \n",
        "                                                       param_name = \"max_depth\", \n",
        "                                                       param_range = param_range, \n",
        "                                                       cv = i_CV, \n",
        "                                                       scoring = \"accuracy\", \n",
        "                                                       n_jobs = -1)\n",
        "\n",
        "# Calculate mean and standard deviation for training set a_scores_CV\n",
        "train_mean = np.mean(train_a_scores_CV, axis = 1)\n",
        "train_std = np.std(train_a_scores_CV, axis = 1)\n",
        "\n",
        "# Calculate mean and standard deviation for test set a_scores_CV\n",
        "test_mean = np.mean(test_a_scores_CV, axis = 1)\n",
        "test_std = np.std(test_a_scores_CV, axis = 1)\n",
        "\n",
        "# Plot mean accuracy a_scores_CV for training and test sets\n",
        "plt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\n",
        "plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n",
        "\n",
        "# Plot accurancy bands for training and test sets\n",
        "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color=\"gray\")\n",
        "plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color=\"gainsboro\")\n",
        "\n",
        "# Create plot\n",
        "plt.title(\"Validation Curve With Random Forest\")\n",
        "plt.xlabel(\"Number Of Trees\")\n",
        "plt.ylabel(\"Accuracy Score\")\n",
        "plt.tight_layout()\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm_fPGYwkJYc"
      },
      "source": [
        "param_range = np.arange(1, 250, 2)\n",
        "\n",
        "# Calculate accuracy on training and test set using range of parameter values\n",
        "train_a_scores_CV, test_a_scores_CV = validation_curve(RandomForestClassifier(), \n",
        "                                             X_treinamento, \n",
        "                                             y_treinamento, \n",
        "                                             param_name='min_samples_leaf', \n",
        "                                             param_range=param_range,\n",
        "                                             cv = i_CV, \n",
        "                                             scoring=\"accuracy\", \n",
        "                                             n_jobs=-1)\n",
        "\n",
        "\n",
        "# Calculate mean and standard deviation for training set a_scores_CV\n",
        "train_mean = np.mean(train_a_scores_CV, axis = 1)\n",
        "train_std = np.std(train_a_scores_CV, axis = 1)\n",
        "\n",
        "# Calculate mean and standard deviation for test set a_scores_CV\n",
        "test_mean = np.mean(test_a_scores_CV, axis = 1)\n",
        "test_std = np.std(test_a_scores_CV, axis = 1)\n",
        "\n",
        "# Plot mean accuracy a_scores_CV for training and test sets\n",
        "plt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\n",
        "plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n",
        "\n",
        "# Plot accurancy bands for training and test sets\n",
        "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color=\"gray\")\n",
        "plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color=\"gainsboro\")\n",
        "\n",
        "# Create plot\n",
        "plt.title(\"Validation Curve With Random Forest\")\n",
        "plt.xlabel(\"Number Of Trees\")\n",
        "plt.ylabel(\"Accuracy Score\")\n",
        "plt.tight_layout()\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAqdiSaVlAB8"
      },
      "source": [
        "param_range = np.arange(0.05, 1, 0.05)\n",
        "\n",
        "# Calculate accuracy on training and test set using range of parameter values\n",
        "train_a_scores_CV, test_a_scores_CV = validation_curve(RandomForestClassifier(), \n",
        "                                             X_treinamento, \n",
        "                                             y_treinamento, \n",
        "                                             param_name='min_samples_split', \n",
        "                                             param_range=param_range,\n",
        "                                             cv = i_CV, \n",
        "                                             scoring=\"accuracy\", \n",
        "                                             n_jobs=-1)\n",
        "\n",
        "\n",
        "# Calculate mean and standard deviation for training set a_scores_CV\n",
        "train_mean = np.mean(train_a_scores_CV, axis = 1)\n",
        "train_std = np.std(train_a_scores_CV, axis = 1)\n",
        "\n",
        "# Calculate mean and standard deviation for test set a_scores_CV\n",
        "test_mean = np.mean(test_a_scores_CV, axis = 1)\n",
        "test_std = np.std(test_a_scores_CV, axis = 1)\n",
        "\n",
        "# Plot mean accuracy a_scores_CV for training and test sets\n",
        "plt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\n",
        "plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n",
        "\n",
        "# Plot accurancy bands for training and test sets\n",
        "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color=\"gray\")\n",
        "plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color=\"gainsboro\")\n",
        "\n",
        "# Create plot\n",
        "plt.title(\"Validation Curve With Random Forest\")\n",
        "plt.xlabel(\"Number Of Trees\")\n",
        "plt.ylabel(\"Accuracy Score\")\n",
        "plt.tight_layout()\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX_gfsbQSdNd"
      },
      "source": [
        "___\n",
        "# **BOOSTING MODELS**\n",
        "* São algoritmos muito utilizados nas competições do Kaggle;\n",
        "* São algoritmos utilizados para melhorar a performance dos algoritmos de Machine Learning;\n",
        "* Modelos:\n",
        "    - [X] AdaBoost\n",
        "    - [X] XGBoost\n",
        "    - [X] LightGBM\n",
        "    - [X] GradientBoosting\n",
        "    - [X] CatBoost\n",
        "\n",
        "## Bagging vs Boosting vc Stacking\n",
        "### **Bagging**\n",
        "* Objetivo é reduzir a variância;\n",
        "\n",
        "#### Como funciona\n",
        "* Seleciona várias amostras **COM REPOSIÇÃO** do dataframe de treinamento. Cada amostra é usada para treinar um modelo usando Decision Trees. Como resultado, temos um ensemble de muitas e diferentes modelos (Decision Trees). A média de desses muitos e diferentes modelos (Decision Trees) são usados para produzir o resultado final;\n",
        "* O resultado final é mais robusto do que usarmos uma simples Decision Tree.\n",
        "\n",
        "![Bagging](https://github.com/MathMachado/Materials/blob/master/Bagging.png?raw=true)\n",
        "\n",
        "Souce: [Boosting and Bagging: How To Develop A Robust Machine Learning Algorithm](https://hackernoon.com/how-to-develop-a-robust-algorithm-c38e08f32201).\n",
        "\n",
        "#### Steps\n",
        "* Suponha um dataframe X_treinamento (dataframe de treinamento) contendo N observações (instâncias, pontos, linhas) e M COLUNAS (features, atributos).\n",
        "    1. Bagging seleciona aleatoriamente uma amostra **COM REPOSIÇÃO** de X_treinamento;\n",
        "    2. Bagging seleciona aleatoriamente M2 (M2 < M) COLUNAS do dataframe extraído do passo (1);\n",
        "    3. Constroi uma Decision Tree com as M2 COLUNAS do passo (2) e o dataframe obtido no passo (1) e as COLUNAS são avaliadas pela sua habilidade de classificar as observações;\n",
        "    4. Os passos (1)--> (2)-- (3) são repetidos K vezes (ou seja, K Decision Trees), de forma que as COLUNAS são ranqueadas pelo seu poder preditivo e o resultado final (acurácia, por exemplo) é obtido pela agregação das predições dos K Decision Trees.\n",
        "\n",
        "#### Vantagens\n",
        "* Reduz overfitting;\n",
        "* Lida bem com dataframes com muitas COLUNAS (high dimensionality);\n",
        "* Lida automaticamente com Missing Values;\n",
        "\n",
        "#### Desvantagem\n",
        "* A predição final é baseada na média das K Decision Trees, o que pode comprometer a acurácia final.\n",
        "\n",
        "___        \n",
        "### **Boosting**\n",
        "* Objetivo é melhorar acurácia;\n",
        "\n",
        "#### Como funciona\n",
        "* Os classificadores são usados sequencialmente, de forma que o classificador no passo N aprende com os erros do classificador do passo N-1. Ou seja, o objetivo é melhorar a precisão/acurácia à cada passo aprendendo com o passado.\n",
        "\n",
        "![Boosting](https://github.com/MathMachado/Materials/blob/master/Boosting.png?raw=true)\n",
        "\n",
        "Source: [Ensemble methods: bagging, boosting and stacking](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205), Joseph Rocca\n",
        ".\n",
        "\n",
        "#### Steps\n",
        "* Suponha um dataframe X_treinamento (dataframe de treinamento) contendo N observações (instâncias, pontos, linhas) e M COLUNAS (features, atributos).\n",
        "    1. Boosting seleciona aleatoriamente uma amostra D1 SEM reposição de X_treinamento;\n",
        "    2. Boosting treina o classificador C1;\n",
        "    3. Boosting seleciona aleatoriamente a SEGUNDA amostra D2 SEM reposição de X_treinamento e acrescenta à D2 50% das observações que foram classificadas incorretamente para treinar o classificador C2;\n",
        "    4. Boosting encontra em X_treinamento a amostra D3 que os classificadores C1 e C2 discordam em classificar e treina C3;\n",
        "    5. Combina (voto) as predições de C1, C2 e C3 para produzir o resultado final.\n",
        "\n",
        "#### Vantagens\n",
        "* Lida bem com dataframes com muitas COLUNAS (high dimensionality);\n",
        "* Lida automaticamente com Missing Values;\n",
        "\n",
        "#### Desvantagem\n",
        "* Propenso a overfitting. Recomenda-se tratar outliers previamente.\n",
        "* Requer ajuste cuidadoso dos hyperparameters;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fgUrkmPk4dr"
      },
      "source": [
        "___\n",
        "# STACKING\n",
        "\n",
        "![Stacking](https://github.com/MathMachado/Materials/blob/master/Stacking.png?raw=true)\n",
        "\n",
        "Kd a referência desta figura???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0jxx3ETpOdm"
      },
      "source": [
        "___\n",
        "# **BOOTSTRAPPING METHODS**\n",
        "> Antes de falarmos de Boosting ou Bagging, precisamos entender primeiro o que é Bootstrap, pois ambos (Boosting e Bagging) são baseados em Bootstrap.\n",
        "\n",
        "* Em Estatística (e em Machine Learning), Bootstrap se refere à extrair amostras aleatórias COM reposição da população X."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyqazmUuifkE"
      },
      "source": [
        "___\n",
        "# **ADABOOST(Adaptive Boosting)**\n",
        "* Quando nada funciona, AdaBoost funciona!\n",
        "* Foi um dos primeiros algoritmos de Boosting (1995);\n",
        "* AdaBoost pode ser utilizado tanto para classificação (AdaBoostClassifier) quanto para Regressão (AdaBoostRegressor);\n",
        "* AdaBoost usam algoritmos DecisionTree como base_estimator;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU-vzkXqrFVw"
      },
      "source": [
        "## Referências\n",
        "* [AdaBoost Classifier Example In Python](https://towardsdatascience.com/machine-learning-part-17-boosting-algorithms-adaboost-in-python-d00faac6c464) - Didático e explica exatamente como o AdaBoost funciona.\n",
        "* [Adaboost for Dummies: Breaking Down the Math (and its Equations) into Simple Terms](https://towardsdatascience.com/adaboost-for-dummies-breaking-down-the-math-and-its-equations-into-simple-terms-87f439757dcf) - Para quem quer entender a matemática por trás do algoritmo.\n",
        "* [Gradient Boosting and XGBoost](https://medium.com/hackernoon/gradient-boosting-and-xgboost-90862daa6c77)\n",
        "* [Understanding AdaBoost](https://towardsdatascience.com/understanding-adaboost-2f94f22d5bfe), Akash Desarda.\n",
        "* [AdaBoost Classifier Example In Python](https://towardsdatascience.com/machine-learning-part-17-boosting-algorithms-adaboost-in-python-d00faac6c464)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EMrjQDZIMl_"
      },
      "source": [
        "## O que é AdaBoost (Adaptive Boosting)?\n",
        "* é um dos classificadores do tipo ensemble (combina vários classificadores para aumentar a precisão).\n",
        "* AdaBoost é um classificador iterativo e forte que combina (ensemble) vários classificadores fracos para melhorar a precisão.\n",
        "* Qualquer algoritmo de aprendizado de máquina pode ser usado como um classificador de base (parâmetro base_estimator);\n",
        "\n",
        "## Parâmetros mais importantes do AdaBoost:\n",
        "* base_estimator - É um classificador usado para treinar o modelo. Como default, AdaBoost usa o DecisionTreeClassifier. Como dito anteriormente, pode-se utilizar diferentes algoritmos para esse fim.\n",
        "* n_estimators - Número de base_estimator para treinar iterativamente.\n",
        "* learning_rate - Controla a contribuição do base_estimator na solução/combinação final;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzLtHzWNJBix"
      },
      "source": [
        "## Usando diferentes algoritmos para base_estimator\n",
        "> Como dito anteriormente, pode-se utilizar vários tipos de base_estimator em AdaBoost. Por exemplo, se quisermos usar SVM (Support Vector Machines), devemos proceder da seguinte forma:\n",
        "\n",
        "\n",
        "```\n",
        "# Importar a biblioteca base_estimator\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Treina o classificador (algoritmo)\n",
        "ml_SVC= SVC(probability=True, kernel='linear')\n",
        "\n",
        "# Constroi o modelo AdaBoost\n",
        "ml_AB = AdaBoostClassifier(n_estimators= 50, base_estimator=ml_SVC, learning_rate=1)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrj4a4s6hMMB"
      },
      "source": [
        "## Vantagens\n",
        "* AdaBoost é fácil de implementar;\n",
        "* AdaBoost corrige os erros do base_estimator iterativamente e melhora a acurácia;\n",
        "* Faz o Feature Selection automaticamente (**Porque**?);\n",
        "* Pode-se usar muitos algoritos como base_estimator ;\n",
        "* Como é um método ensemble, então o modelo final é pouco propenso à overfitting.\n",
        "\n",
        "## Desvantagens\n",
        "* AdaBoost é sensível a ruídos nos dados;\n",
        "* Altamente impactado por outliers (contribui para overfitting), pois o algoritmo tenta se ajustr a cada ponto da mehor forma possível;\n",
        "* AdaBoost é mais lento que XGBoost;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgJmu7YLiyv7"
      },
      "source": [
        "No exemplo a seguir, vou usar RandomForestClassifier com os parâmetros otimizados, ou seja:\n",
        "\n",
        "```\n",
        "best_params= {'bootstrap': False, 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 400}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VCRNyZT3qvc"
      },
      "source": [
        "best_params= {'bootstrap': False, 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 400}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gIboJdriq61"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Instancia RandomForestClassifier - Parâmetros otimizados!\n",
        "ml_RF2= RandomForestClassifier(bootstrap= best_params['bootstrap'], \n",
        "                                  max_depth= best_params['max_depth'], \n",
        "                                  max_features= best_params['max_features'], \n",
        "                                  min_samples_leaf= best_params['min_samples_leaf'], \n",
        "                                  min_samples_split= best_params['min_samples_split'], \n",
        "                                  n_estimators= best_params['n_estimators'], \n",
        "                                  random_state= i_Seed)\n",
        "# Instancia AdaBoostClassifier\n",
        "ml_AB= AdaBoostClassifier(n_estimators=100, base_estimator= ml_RF2, random_state= i_Seed)\n",
        "\n",
        "# Treina...\n",
        "ml_AB.fit(X_treinamento, y_treinamento)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBOuTywWRm91"
      },
      "source": [
        "# Cross-Validation com 10 folds\n",
        "a_scores_CV = funcao_cross_val_score(ml_AB, X_treinamento, y_treinamento, i_CV)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7Ce5L38ECoC"
      },
      "source": [
        "**Interpretação**: Nosso classificador (AdaBoostClassifier) tem uma acurácia média de 96,72% (base de treinamento). Além disso, o std é da ordem de 2,54%, ou seja, pequena. Vamos tentar melhorar a acurácia do classificador usando parameter tunning (GridSearchCV)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5GfnBwEifkO"
      },
      "source": [
        "print(f'Acurácias: {a_scores_CV}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9rSpuXyEPA5"
      },
      "source": [
        "# Faz predições com os parametros otimizados...\n",
        "y_pred = ml_AB.predict(X_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F9k-_eXGDLa"
      },
      "source": [
        "# Confusion Matrix\n",
        "cf_matrix = confusion_matrix(y_teste, y_pred)\n",
        "cf_labels = ['True_Negative','False_Positive','False_Negative','True_Positive']\n",
        "cf_categories = ['Zero', 'One']\n",
        "mostra_confusion_matrix(cf_matrix, group_names= cf_labels, categories= cf_categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XweWTjQ9EXLw"
      },
      "source": [
        "## Parameter tunning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcrKzse9EbL_"
      },
      "source": [
        "# Dicionário de parâmetros para o parameter tunning.\n",
        "d_hiperparametros_AB = {'n_estimators':[50, 100, 200], 'learning_rate':[.001, 0.01, 0.05, 0.1, 0.3,1]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Susc3I7mFDQX"
      },
      "source": [
        "# Invoca a função\n",
        "ml_AB2, best_params= GridSearchOptimizer(ml_AB, 'ml_AB2', d_hiperparametros_AB, X_treinamento, y_treinamento, X_teste, y_teste, i_CV, l_colunas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4JjWsusjNS8"
      },
      "source": [
        "___\n",
        "# **GRADIENT BOOSTING**\n",
        "* Gradient boosting pode ser usado para resolver problemas de classificação (GradientBoostingClassifier) e Regressão (GradientBoostingRegressor);\n",
        "* Gradient boosting são um refinamento do AdaBoost (lembra que AdaBoost foi um dos primeiros métodos de Boosting - criado em 1995). O que Gradient Boosting faz adicionalmente ao AdaBoost é minimizar a loss (função perda), ie, minimizar a diferença entre os valores observados de y e os valores preditos.\n",
        "* Usa Gradient Descent para encontrar as deficiências nas previsões do passo anterior. Gradient Descent é um algoritmo popular e poderoso e usado em Redes Neurais;\n",
        "* O objetivo do Gradient Boosting é minimizar 'loss function'. Portanto, Gradient Boosting depende da \"loss function\".\n",
        "* Gradient boosting usam algoritmos DecisionTree como base_estimator;\n",
        "\n",
        "## Vantagens\n",
        "* Não há necessidade de pre-processing;\n",
        "* Trabalha normalmente com COLUNAS numéricas ou categóricas;\n",
        "* Trata automaticamente os Missing Values. Ou seja, não é necessário aplicar métodos de Missing Value Imputation;\n",
        "\n",
        "## Desvantagens\n",
        "* Como Gradient Boosting tenta continuamente minimizar os erros à cada iteração, isso pode enfatizar os outliers e causar overfitting. Portanto, deve-se:\n",
        "    * Tratar os outliers previamente OU\n",
        "    * Usar Cross-Validation para neutralizar os efeitos dos outliers (**Eu prefiro este método, pois toma menos tempo**);\n",
        "* Computacionalmene caro. Geralmente são necessários muitas árvores (> 1000) para se obter bons resultados;\n",
        "* Devido à flexibilidade (muitos parâmetros para ajustar), então é necessário usar GridSearchCV para encontrar a combinação ótima dos hyperparameters;\n",
        "\n",
        "## Referências\n",
        "* [Gradient Boosting Decision Tree Algorithm Explained](https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4) - Didático e detalhista.\n",
        "* [Predicting Wine Quality with Gradient Boosting Machines](https://towardsdatascience.com/predicting-wine-quality-with-gradient-boosting-machines-a-gmb-tutorial-d950b1542065)\n",
        "* [Parameter Tuning in Gradient Boosting (GBM) with Python](https://www.datacareer.de/blog/parameter-tuning-in-gradient-boosting-gbm/)\n",
        "* [Tune Learning Rate for Gradient Boosting with XGBoost in Python](https://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/)\n",
        "* [In Depth: Parameter tuning for Gradient Boosting](https://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae) - Muito bom\n",
        "* [Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting (GBM) in Python](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4bUCZs2jNTA"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Instancia...\n",
        "ml_GB = GradientBoostingClassifier(n_estimators = 100, min_samples_split = 2)\n",
        "\n",
        "# Treina... \n",
        "ml_GB.fit(X_treinamento, y_treinamento)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKOG1ugSRvLM"
      },
      "source": [
        "# Cross-Validation com 10 folds\n",
        "a_scores_CV = funcao_cross_val_score(ml_GB, X_treinamento, y_treinamento, i_CV)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlC3y3M5YaGG"
      },
      "source": [
        "print(f'Acurácias: {a_scores_CV}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnLvQ0ZDYNjB"
      },
      "source": [
        "**Interpretação**: Nosso classificador (GradientBoostingClassifier) tem uma acurácia média de 96,86% (base de treinamento). Além disso, o std é da ordem de 2,52%, ou seja, pequena. Vamos tentar melhorar a acurácia do classificador usando parameter tunning (GridSearchCV)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2n1RKZuXq3D"
      },
      "source": [
        "# Faz precições...\n",
        "y_pred = ml_GB.predict(X_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r6JCzQRGFa0"
      },
      "source": [
        "# Confusion Matrix\n",
        "cf_matrix = confusion_matrix(y_teste, y_pred)\n",
        "cf_labels = ['True_Negative','False_Positive','False_Negative','True_Positive']\n",
        "cf_categories = ['Zero', 'One']\n",
        "mostra_confusion_matrix(cf_matrix, group_names = cf_labels, categories = cf_categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFv-Q2AD5uCk"
      },
      "source": [
        "## Parameter tunning\n",
        "> Consulte [Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting (GBM) in Python](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/) para detalhes sobre os parâmetros, significado e etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgU040AcjNTF"
      },
      "source": [
        "# Dicionário de parâmetros para o parameter tunning.\n",
        "d_hiperparametros_GB= {'learning_rate': [1, 0.5, 0.25, 0.1, 0.05, 0.01]} #,\n",
        "#                  'n_estimators': [1, 2, 4, 8, 16, 32, 64, 100, 200],\n",
        "#                  'max_depth': [5, 10, 15, 20, 25, 30],\n",
        "#                  'min_samples_split': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "#                  'min_samples_leaf': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "#                  'max_features': list(range(1, X_treinamento.shape[1]))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5KLFlpTjNTH"
      },
      "source": [
        "# Invoca a função\n",
        "ml_GB2, best_params= GridSearchOptimizer(ml_GB, 'ml_GB2', d_hiperparametros_GB, X_treinamento, y_treinamento, X_teste, y_teste, i_CV, l_colunas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ6ERz3fi9i2"
      },
      "source": [
        "### Resultado da execução do Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSa7uKw13mKG"
      },
      "source": [
        "```\n",
        "[Parallel(n_jobs=-1)]: Done 275400 out of 275400 | elapsed: 93.7min finished\n",
        "\n",
        "Parametros otimizados: {'learning_rate': 1, 'max_depth': 30, 'max_features': 11, 'min_samples_leaf': 0.1, 'min_samples_split': 0.1, 'n_estimators': 100}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiJpA2PyjDjR"
      },
      "source": [
        "# Como o procedimento acima levou 93 minutos para executar, então vou estimar ml_GB2 abaixo usando os parâmetros acima estimados\n",
        "best_params= {'learning_rate': 1, 'max_depth': 30, 'max_features': 11, 'min_samples_leaf': 0.1, 'min_samples_split': 0.1, 'n_estimators': 100}\n",
        "\n",
        "#ml_GB2= GradientBoostingClassifier(learning_rate= best_params['learning_rate'], \n",
        "#                                      max_depth= best_params['max_depth'],\n",
        "#                                      max_features= best_params['max_features'],\n",
        "#                                      min_samples_leaf= best_params['min_samples_leaf'],\n",
        "#                                      min_samples_split= best_params['min_samples_split'],\n",
        "#                                      n_estimators= best_params['n_estimators'],\n",
        "#                                      random_state= i_Seed)\n",
        "\n",
        "ml_GB2= GradientBoostingClassifier(learning_rate= best_params['learning_rate'], \n",
        "                                      max_depth= best_params['max_depth'],\n",
        "                                      min_samples_leaf= best_params['min_samples_leaf'],\n",
        "                                      min_samples_split= best_params['min_samples_split'],\n",
        "                                      n_estimators= best_params['n_estimators'],\n",
        "                                      random_state= i_Seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb14gJ7-jbVM"
      },
      "source": [
        "## Selecionar as COLUNAS importantes/relevantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAqGZIFYm2sU"
      },
      "source": [
        "X_treinamento_GB, X_teste_GB = seleciona_colunas_relevantes(ml_GB2, X_treinamento, X_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yiu6dahnBvC"
      },
      "source": [
        "## Treina o classificador com as COLUNAS relevantes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APrtWN18nc4t"
      },
      "source": [
        "best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VS0mLdOmnXAY"
      },
      "source": [
        "# Treina com as COLUNAS relevantes\n",
        "ml_GB2.fit(X_treinamento_GB, y_treinamento)\n",
        "\n",
        "# Cross-Validation com 10 folds\n",
        "a_scores_CV = funcao_cross_val_score(ml_GB2, X_treinamento_GB, y_treinamento, i_CV)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmc9PP_Rn1TN"
      },
      "source": [
        "## Valida o modelo usando o dataframe X_teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3mnIALvnzP2"
      },
      "source": [
        "y_pred_GB = ml_GB2.predict(X_teste_GB)\n",
        "\n",
        "# Calcula acurácia\n",
        "accuracy_score(y_teste, y_pred_GB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwP9Z2GnkV7r"
      },
      "source": [
        "___\n",
        "# **XGBOOST (eXtreme Gradient Boosting)**\n",
        "* XGBoost é uma melhoria de Gradient Boosting. As melhorias são em velocidade e performace, além de corrigir as ineficiências do GradientBoosting.\n",
        "* Algoritmo preferido pelos Kaggle Grandmasters;\n",
        "* Paralelizável;\n",
        "* Estado-da-arte em termos de Machine Learning;\n",
        "\n",
        "## Parâmetros relevantes e seus valores iniciais\n",
        "Consulte [Complete Guide to Parameter Tuning in XGBoost with codes in Python](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/) para detalhes completos sobre os parâmetros, significado e etc.\n",
        "\n",
        "* n_estimators = 100 (100 caso o dataframe for grande. Se o dataframe for médio/pequeno, então 1000) - É o número de árvores desejamos construir;\n",
        "* max_depth= 3 - Determina quão profundo cada árvore pode crescer durante qualquer round de treinamento. Valores típicos no intervalo [3, 10];\n",
        "* learning rate= 0.01 - Usado para evitar overfitting, intervalo: [0, 1];\n",
        "* alpha (somente para problemas de Regressão) - L1 regularization nos pesos. Valores altos resulta em mais regularization;\n",
        "* lambda (somente para problemas de Regressão) - L2 regularization nos pesos.\n",
        "* colsample_bytree: 1 - porcentagem de COLUNAS usados por cada árvore. Alto valor pode causar overfitting;\n",
        "* subsample: 0.8 - porcentagem de amostras usadas por árvore. Um valor baixo pode levar a overfitting;\n",
        "* gamma: 1 - Controla se um determinado nó será dividido com base na redução esperada na perda após a divisão. Um valor mais alto leva a menos divisões.\n",
        "* objective: Define a \"loss function\". As opções são:\n",
        "    * reg:linear - Para resolver problemas de regressão;\n",
        "    * reg:logistic - Para resolver problemas de classificação;\n",
        "    * binary:logistic - Para resolver problemas de classificação com cálculo de probabilidades;\n",
        "\n",
        "# Referências\n",
        "* [How exactly XGBoost Works?](https://medium.com/@pushkarmandot/how-exactly-xgboost-works-a320d9b8aeef)\n",
        "* [Fine-tuning XGBoost in Python like a boss](https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e)\n",
        "* [Gentle Introduction of XGBoost Library](https://medium.com/@imoisharma/gentle-introduction-of-xgboost-library-2b1ac2669680)\n",
        "* [A Beginner’s guide to XGBoost](https://towardsdatascience.com/a-beginners-guide-to-xgboost-87f5d4c30ed7)\n",
        "* [Exploring XGBoost](https://towardsdatascience.com/exploring-xgboost-4baf9ace0cf6)\n",
        "* [Feature Importance and Feature Selection With XGBoost in Python](https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/)\n",
        "* [Ensemble Learning case study: Running XGBoost on Google Colab free GPU](https://towardsdatascience.com/running-xgboost-on-google-colab-free-gpu-a-case-study-841c90fef101) - Recomendo\n",
        "* [Predicting movie revenue with AdaBoost, XGBoost and LightGBM](https://towardsdatascience.com/predicting-movie-revenue-with-adaboost-xgboost-and-lightgbm-262eadee6daa)\n",
        "* [Tuning XGBoost Hyperparameters with Scikit Optimize](https://towardsdatascience.com/how-to-improve-the-performance-of-xgboost-models-1af3995df8ad)\n",
        "* [An Example of Hyperparameter Optimization on XGBoost, LightGBM and CatBoost using Hyperopt](https://towardsdatascience.com/an-example-of-hyperparameter-optimization-on-xgboost-lightgbm-and-catboost-using-hyperopt-12bc41a271e) - Interessante\n",
        "* [XGBOOST vs LightGBM: Which algorithm wins the race !!!](https://towardsdatascience.com/lightgbm-vs-xgboost-which-algorithm-win-the-race-1ff7dd4917d) - LightGBM tem se mostrado interessante.\n",
        "* [From Zero to Hero in XGBoost Tuning](https://towardsdatascience.com/from-zero-to-hero-in-xgboost-tuning-e48b59bfaf58) - Gostei\n",
        "* [Build XGBoost / LightGBM models on large datasets — what are the possible solutions?](https://towardsdatascience.com/build-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d)\n",
        "* [Selecting Optimal Parameters for XGBoost Model Training](https://towardsdatascience.com/selecting-optimal-parameters-for-xgboost-model-training-c7cd9ed5e45e) - Muito bom!\n",
        "* [CatBoost vs. Light GBM vs. XGBoost](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMM_R4_ukV7x"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "# Instancia...\n",
        "ml_XGB= XGBClassifier(silent=False, \n",
        "                         scale_pos_weight=1,\n",
        "                        learning_rate=0.01,  \n",
        "                        colsample_bytree = 1,\n",
        "                        subsample = 0.8,\n",
        "                        objective='binary:logistic', \n",
        "                        n_estimators=1000, \n",
        "                        reg_alpha = 0.3,\n",
        "                        max_depth= 3, \n",
        "                        gamma=1, \n",
        "                        max_delta_step=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4wQMlDEFINR"
      },
      "source": [
        "# Treina...\n",
        "ml_XGB.fit(X_treinamento, y_treinamento)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S77LljiQR_16"
      },
      "source": [
        "# Cross-Validation com 10 folds\n",
        "a_scores_CV = funcao_cross_val_score(ml_XGB, X_treinamento, y_treinamento, i_CV)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNyKX6PkrXOk"
      },
      "source": [
        "**Interpretação**: Nosso classificador (XGBClassifier) tem uma acurácia média de 96,72% (base de treinamento). Além disso, o std é da ordem de 2,02%, ou seja, pequena. Vamos tentar melhorar a acurácia do classificador usando parameter tunning (GridSearchCV)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h0QYv3FkV73"
      },
      "source": [
        "print(f'Acurácias: {a_scores_CV}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKhhAZLjkV76"
      },
      "source": [
        "# Faz predições...\n",
        "y_pred = ml_XGB.predict(X_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir2Kd1PqGHgz"
      },
      "source": [
        "# Confusion Matrix\n",
        "cf_matrix = confusion_matrix(y_teste, y_pred)\n",
        "cf_labels = ['True_Negative','False_Positive','False_Negative','True_Positive']\n",
        "cf_categories = ['Zero', 'One']\n",
        "mostra_confusion_matrix(cf_matrix, group_names= cf_labels, categories= cf_categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEC7gW4qYpWw"
      },
      "source": [
        "## Parameter tunning\n",
        "### Leitura Adicional:\n",
        "* [Fine-tuning XGBoost in Python like a boss](https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e)\n",
        "* [Complete Guide to Parameter Tuning in XGBoost with codes in Python](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n",
        "\n",
        "> Olhando para os resultados acima, qual o melhor modelo?\n",
        "\n",
        "XGBoost? Supondo que sim, agora vamos fazer o fine-tuning dos parâmetros do modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3MsUONPwIV9"
      },
      "source": [
        "# Dicionário de parâmetros para XGBoost:\n",
        "d_hiperparametros_XGB = {'min_child_weight': [i for i in np.arange(1, 13)]} #,\n",
        "#                'gamma': [i for i in np.arange(0, 5, 0.5)],\n",
        "#                'subsample': [0.6, 0.8, 1.0],\n",
        "#                'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "#                'max_depth': [3, 4, 5, 7, 9],\n",
        "#                'learning_rate': [i for i in np.arange(0.01, 1, 0.1)]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX27FCKmwSni"
      },
      "source": [
        "# Invoca a função\n",
        "ml_XGB, best_params= GridSearchOptimizer(ml_XGB, 'ml_XGB2', d_hiperparametros_XGB, X_treinamento, y_treinamento, X_teste, y_teste, i_CV, l_colunas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b7uCuF74Hjv"
      },
      "source": [
        "### Resultado da execução do XGBoostClassifier\n",
        "\n",
        "```\n",
        "[Parallel(n_jobs=-1)]: Done 108000 out of 108000 | elapsed: 372.0min finished\n",
        "\n",
        "Parametros otimizados: {'colsample_bytree': 0.8, 'gamma': 0.5, 'learning_rate': 0.51, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7E0oyxEtbGi"
      },
      "source": [
        "# Como o procedimento acima levou 372 minutos para executar, então vou estimar ml_XGB2 abaixo usando os parâmetros acima estimados\n",
        "best_params= {'colsample_bytree': 0.8, 'gamma': 0.5, 'learning_rate': 0.51, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6}\n",
        "\n",
        "ml_XGB2= XGBClassifier(min_child_weight= best_params['min_child_weight'], \n",
        "                       gamma= best_params['gamma'], \n",
        "                       subsample= best_params['subsample'], \n",
        "                       colsample_bytree= best_params['colsample_bytree'], \n",
        "                       max_depth= best_params['max_depth'], \n",
        "                       learning_rate= best_params['learning_rate'], \n",
        "                       random_state= i_Seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuqyLHTU5Z-j"
      },
      "source": [
        "## Selecionar as COLUNAS importantes/relevantes\n",
        "* [The Multiple faces of ‘Feature importance’ in XGBoost](https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPG3JZIpRZ-T"
      },
      "source": [
        "# plot feature importance\n",
        "from xgboost import plot_importance\n",
        "\n",
        "xgb.plot_importance(ml_XGB2, color = 'red')\n",
        "plt.title('importance', fontsize = 20)\n",
        "plt.yticks(fontsize = 10)\n",
        "plt.ylabel('features', fontsize = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmpRC2lHW-KP"
      },
      "source": [
        "ml_XGB2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f9MIEBiyq-5"
      },
      "source": [
        "X_treinamento_XGB, X_teste_XGB= seleciona_colunas_relevantes(ml_XGB2, X_treinamento, X_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6EayWaY5nMm"
      },
      "source": [
        "## Treina o classificador com as COLUNAS relevantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Huy18gKI5qad"
      },
      "source": [
        "best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3-PaTdc5vZk"
      },
      "source": [
        "# Treina com as COLUNAS relevantes...\n",
        "ml_XGB2.fit(X_treinamento_XGB, y_treinamento)\n",
        "\n",
        "# Cross-Validation com 10 folds\n",
        "a_scores_CV = funcao_cross_val_score(ml_XGB2, X_treinamento_XGB, y_treinamento, i_CV)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBdYikDU6NhD"
      },
      "source": [
        "## Valida o modelo usando o dataframe X_teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcvY-VdL6VIZ"
      },
      "source": [
        "y_pred_XGB = ml_XGB2.predict(X_teste_XGB)\n",
        "\n",
        "# Calcula acurácia\n",
        "accuracy_score(y_teste, y_pred_XGB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oLtdH-vTSbC"
      },
      "source": [
        "xgb.to_graphviz(ml_XGB2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czXQG3MCHfHM"
      },
      "source": [
        "# KNN - KNEIGHBORSCLASSIFIER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llTTXNeyHiwx"
      },
      "source": [
        "# BAGGINGCLASSIFIER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbkekd4QHoZO"
      },
      "source": [
        "# EXTRATREESCLASSIFIER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "widavwR4HzwE"
      },
      "source": [
        "# SVM\n",
        "https://data-flair.training/blogs/svm-support-vector-machine-tutorial/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id_Ubulns6We"
      },
      "source": [
        "# NAIVE BAYES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycu_EIGlYUYn"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from xgboost              import XGBClassifier\n",
        "from sklearn.ensemble     import ExtraTreesClassifier\n",
        "from sklearn.tree         import ExtraTreeClassifier\n",
        "from sklearn.tree         import DecisionTreeClassifier\n",
        "from sklearn.ensemble     import GradientBoostingClassifier\n",
        "from sklearn.ensemble     import BaggingClassifier\n",
        "from sklearn.ensemble     import AdaBoostClassifier\n",
        "from sklearn.ensemble     import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from lightgbm             import LGBMClassifier\n",
        "\n",
        "clfs = [XGBClassifier(),              LGBMClassifier(), \n",
        "        ExtraTreesClassifier(),       ExtraTreeClassifier(),\n",
        "        BaggingClassifier(),          DecisionTreeClassifier(),\n",
        "        GradientBoostingClassifier(), LogisticRegression(),\n",
        "        AdaBoostClassifier(),         RandomForestClassifier()]\n",
        "\n",
        "for clf in clfs:\n",
        "    try:\n",
        "        _ = mostra_feature_importances(clf, X_treinamento, y_treinamento, top_n=X_treinamento.shape[1], title=clf.__class__.__name__)\n",
        "    except AttributeError as e:\n",
        "        print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwWkjfC8KEZH"
      },
      "source": [
        "# ENSEMBLE METHODS\n",
        "https://towardsdatascience.com/using-bagging-and-boosting-to-improve-classification-tree-accuracy-6d3bb6c95e5b\n",
        "\n",
        "![Ensemble](https://github.com/MathMachado/Materials/blob/master/Ensemble.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Uf1RML7xETY"
      },
      "source": [
        "# WOE e IV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBNRfYZCyhMP"
      },
      "source": [
        "## Construção do exemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIIroyyP4ZRZ"
      },
      "source": [
        "df_y.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzQQdrkf1ohX"
      },
      "source": [
        "from random import choices\n",
        "\n",
        "df_X2= df_X.copy()\n",
        "df_X2['tipo']= choices(['A', 'B', 'C', 'D'], k= 1000)\n",
        "df_X2['idade']= np.random.randint(10, 15, size= 1000)\n",
        "df_X2['target']= df_y['target']\n",
        "df_X2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-OpwIpx4hXJ"
      },
      "source": [
        "df_X2['target'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZfqSvbKzeJ3"
      },
      "source": [
        "def Constroi_Buckets(df, i, k= 10):\n",
        "    coluna= 'v'+ str(i)\n",
        "    df[coluna+'_Bucket']= pd.cut(df[coluna], bins= k, labels= np.arange(1, k+1))\n",
        "    df= df.drop(columns= [coluna], axis= 1)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6Nrpsx60HD3"
      },
      "source": [
        "for i in np.arange(1,19):\n",
        "    df_X2= Constroi_Buckets(df_X2, i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2Fbh41-03OB"
      },
      "source": [
        "df_X2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9r5BeWVxIr3"
      },
      "source": [
        "# Função para calcular WOE e IV\n",
        "def calculate_woe_iv(dataset, feature, target):\n",
        "\n",
        "    def codethem(IV):\n",
        "        if  IV < 0.02: return 'Useless'\n",
        "        elif IV >= 0.02 and IV < 0.1: return 'Weak'\n",
        "        elif IV >= 0.1 and IV < 0.3: return 'Medium'\n",
        "        elif IV >= 0.3 and IV < 0.5: return 'Strong'\n",
        "        elif IV >= 0.5: return 'Suspicious'\n",
        "        else: return 'None'\n",
        "\n",
        "    lst = []\n",
        "    for i in range(dataset[feature].nunique()):\n",
        "        val = list(dataset[feature].unique())[i]\n",
        "        lst.append({\n",
        "            'Value': val,\n",
        "            'All': dataset[dataset[feature] == val].count()[feature],\n",
        "            'Good': dataset[(dataset[feature] == val) & (dataset[target] == 0)].count()[feature],\n",
        "            'Bad': dataset[(dataset[feature] == val) & (dataset[target] == 1)].count()[feature]\n",
        "        })\n",
        "        \n",
        "    dset = pd.DataFrame(lst)\n",
        "    dset['Distr_Good'] = dset['Good']/dset['Good'].sum()\n",
        "    dset['Distr_Bad'] = dset['Bad']/dset['Bad'].sum()\n",
        "    dset['Mean']= dset['All']/dset['All'].sum()\n",
        "    dset['WoE'] = np.log(dset['Distr_Good']/dset['Distr_Bad'])\n",
        "    dset = dset.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n",
        "    dset['IV'] = (dset['Distr_Good'] - dset['Distr_Bad']) * dset['WoE']\n",
        "    #dset= dset.drop(columns= ['Distr_Good', 'Distr_Bad'], axis= 1)\n",
        "\n",
        "    dset['Predictive_Power']= dset['IV'].map(codethem)\n",
        "    iv = dset['IV'].sum()    \n",
        "    dset = dset.sort_values(by='IV')    \n",
        "    return dset, iv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8WGjWH63nx_"
      },
      "source": [
        "df_Lab = df_X2.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N6xr1MgxTiz"
      },
      "source": [
        "def calcula_Predictive_Power(df_Lab, coluna):\n",
        "    print('WoE and IV for column: {}'.format(coluna))\n",
        "    df, iv = calculate_woe_iv(df_Lab, coluna, 'target')\n",
        "    print(df)\n",
        "    print('IV score: {:.2f}'.format(iv))\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayqN_7WnxVq9"
      },
      "source": [
        "for i in np.arange(1,19):\n",
        "    coluna= 'v'+str(i)+'_Bucket'\n",
        "    calcula_Predictive_Power(df_Lab, coluna)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtoJVI4Pyx3I"
      },
      "source": [
        "# **IMBALANCED SAMPLE**\n",
        "> Alguns objetivos como detectar fraude em transações bancárias ou detecção de intrusão em network tem em comum o fato que a classe de interesse (o que queremos detectar), geralmente é um evento raro\n",
        "\n",
        "## Exemplo: Detectar fraude\n",
        "A proporção de fraudes diante de NÃO-FRAUDES são mais ou menos 1%/99%. Neste caso, ao desenvovermos um modelo para detectar fraudes e o modelo classificar todas as instâncias como NÃO-FRAUDE, então o modelo terá uma acurácia de 99%. No entanto, este modelo não nos ajudará em nada.\n",
        "\n",
        "## Necessidade de se usar outras métricas \n",
        "> Recomenda-se utilizar outras métricas (na verdade, é boa prática usar mais de 1 métrica para medir a performance dos modelos) como, por exemplo, F1-Score, Precision/Specificity, Recall/Sensitivity e AUROC.\n",
        "\n",
        "## Como lidar com a amostra desbalanceada?\n",
        "* Under-sampling\n",
        "> Seleciona aleatoriamente a classe MAJORITÁRIA (em nosso exemplo, NÃO-FRAUDE) até o número de instâncias da classe MINORITÁRIA (FRAUDE);\n",
        "\n",
        "* Over-sampling\n",
        "> Resample aleatoriamente a classe MINORITÁRIA (em nosso exemplo, FRAUDE) até o número de instâncias da classe MAJORITÁRIA (NÃO-FRAUDE), ou uma proporção da classe MAJORITÁRIA. Veja a bibliotea SMOTE (Synthetic Minority Over-Sampling Techniques);\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o45zx8zw-aB"
      },
      "source": [
        "## EFEITOS DA AMOSTRA DESBALANCEADA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCVTPCB-Xkbd"
      },
      "source": [
        "# TPOT\n",
        "https://towardsdatascience.com/tpot-automated-machine-learning-in-python-4c063b3e5de9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ulXii6JXpWd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TWUq-z4X4yZ"
      },
      "source": [
        "___\n",
        "# FEATURETOOLS\n",
        "https://medium.com/@rrfd/simple-automatic-feature-engineering-using-featuretools-in-python-for-classification-b1308040e183\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2018/08/guide-automated-feature-engineering-featuretools-python/\n",
        "\n",
        "https://mlwhiz.com/blog/2019/05/19/feature_extraction/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZUNOgmSgAmq"
      },
      "source": [
        "!pip install featuretools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sxdONzsh9rb"
      },
      "source": [
        "df_X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5_ynGo1dBJJ"
      },
      "source": [
        "df_X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqJRJXUhiDqf"
      },
      "source": [
        "from random import choices\n",
        "\n",
        "df_X2= df_X.copy()\n",
        "df_X2['tipo'] = choices(['A', 'B', 'C', 'D'], k = 1000)\n",
        "df_X2['idade'] = np.random.randint(10, 15, size = 1000)\n",
        "df_X2['id'] = range(0,1000)\n",
        "df_X2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR56bGGngk-W"
      },
      "source": [
        "# Automated feature engineering\n",
        "import featuretools as ft\n",
        "import featuretools.variable_types as vtypes\n",
        "\n",
        "es= ft.EntitySet(id = 'simulacao')\n",
        "\n",
        "# adding a dataframe \n",
        "es.entity_from_dataframe(entity_id = 'df_X2', dataframe = df_X2, index = 'id')\n",
        "es"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOJ4Tr5Ogk6M"
      },
      "source": [
        "es['df_X2'].variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uXPqHDZgkys"
      },
      "source": [
        "variable_types = {'idade': vtypes.Categorical}\n",
        "                  \n",
        "es.entity_from_dataframe(entity_id = 'df_X2', dataframe = df_X2, index = 'id', variable_types= variable_types)\n",
        "\n",
        "es = es.normalize_entity(base_entity_id='df_X2', new_entity_id= 'tipo', index='id')\n",
        "es = es.normalize_entity(base_entity_id='df_X2', new_entity_id= 'idade', index='id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnbYTBqugkvm"
      },
      "source": [
        "es"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2v_jetdgkr7"
      },
      "source": [
        "feature_matrix, feature_names = ft.dfs(entityset=es, target_entity = 'df_X2', max_depth = 3, verbose = 3, n_jobs= 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZiRBvHXgkoJ"
      },
      "source": [
        "feature_matrix.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWiahwKe2d6U"
      },
      "source": [
        "# **EXERCÍCIOS**\n",
        "> Encontre algoritmos adequados para ser aplicados aos seguintes problemas:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbSLkbDB2mzK"
      },
      "source": [
        "## Exercício 1 - Credit Card Fraud Detection\n",
        "Source: [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)\n",
        "\n",
        "### Leitura suporte\n",
        "* [Detecting Credit Card Fraud Using Machine Learning](https://towardsdatascience.com/detecting-credit-card-fraud-using-machine-learning-a3d83423d3b8)\n",
        "* [Credit Card Fraud Detection](https://towardsdatascience.com/credit-card-fraud-detection-a1c7e1b75f59)\n",
        "\n",
        "### Dataframe\n",
        "* [Creditcard.csv](https://raw.githubusercontent.com/MathMachado/DSWP/master/Dataframes/creditcard.csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYVM3StS-g0E"
      },
      "source": [
        "### Importar as libraries necessárias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyliPChh-jPk"
      },
      "source": [
        "from sklearn.metrics import accuracy_score # para medir a acurácia do modelo preditivo\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix # para plotar a confusion matrix\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV # para otimizar os parâmetros dos modelos preditivos\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from time import time\n",
        "from operator import itemgetter\n",
        "from scipy.stats import randint\n",
        "\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.externals.six import StringIO  \n",
        "from IPython.display import Image  \n",
        "import pydotplus\n",
        "\n",
        "np.set_printoptions(suppress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAl9ZwP_0-d0"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/MathMachado/DSWP/master/Dataframes/creditcard.csv'\n",
        "df_cc = pd.read_csv(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6lN8FjJ12VU"
      },
      "source": [
        "df_cc.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M47GS1cK2NdD"
      },
      "source": [
        "df_cc.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2QBZFbR3W_q"
      },
      "source": [
        "df_cc['Class'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzjW3Bf_3h7J"
      },
      "source": [
        "56/12842"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bWDX9H12k5g"
      },
      "source": [
        "### Drop NaN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27ob8tRR21TB"
      },
      "source": [
        "df_cc.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9k16WLI49JI"
      },
      "source": [
        "df_cc2 = df_cc.copy()\n",
        "df_cc2 = df_cc.dropna()\n",
        "df_cc2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY-DYRKg34ZX"
      },
      "source": [
        "### Definir as variáveis globais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVhHgV_s3_5f"
      },
      "source": [
        "i_CV = 10 # Número de Cross-Validations\n",
        "i_Seed = 20111974 # semente por questões de reproducibilidade\n",
        "f_Test_Size = 0.3 # Proporção do dataframe de validação (outros valores poderiam ser 0.15, 0.20 ou 0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKbqrF4Q2nBq"
      },
      "source": [
        "### Define amostras de treinamento e teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8CUAiA57OhS"
      },
      "source": [
        "df_cc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZjNUDNb7s1t"
      },
      "source": [
        "# Definição do dataframe contendo as variáveis preditoras:\n",
        "df_X = df_cc2.copy()\n",
        "df_X.drop(columns= ['Class'], axis = 1, inplace = True)\n",
        "df_X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3DDsN2V7IOU"
      },
      "source": [
        "df_y = df_cc2['Class'] # Variável-resposta\n",
        "df_y.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMthdXHD8vnh"
      },
      "source": [
        "df_y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiJRftpZ2103"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(df_X, df_y, test_size = f_Test_Size, random_state = i_Seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmSkPzNt8O6I"
      },
      "source": [
        "X_treinamento.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h1PjPKh8Xb1"
      },
      "source": [
        "X_teste.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbCN_puI2qk1"
      },
      "source": [
        "### Ajusta o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjRwSI079ADn"
      },
      "source": [
        "# Importar o classificador (modelo, algoritmo, ...)\n",
        "from sklearn.tree import DecisionTreeClassifier # Este é o nosso classificador"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuhKJOQA22bR"
      },
      "source": [
        "ml_DT = DecisionTreeClassifier(max_depth = 5, min_samples_split = 2, random_state = i_Seed)\n",
        "ml_DT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zai1d6eM93VQ"
      },
      "source": [
        "# Treinar o algoritmo/classificador: fit(df)\n",
        "ml_DT.fit(X_treinamento, y_treinamento)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybbS4zHn-8BO"
      },
      "source": [
        "# Cross-Validation com 10 folds\n",
        "a_scores_CV = funcao_cross_val_score(ml_DT, X_treinamento, y_treinamento, cv = i_CV)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_NLku7q_YT9"
      },
      "source": [
        "a_scores_CV # array com os scores a cada iteração do CV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCRgHxUu2s7c"
      },
      "source": [
        "### Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wMWm-p5229A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am_UELOg2vDh"
      },
      "source": [
        "### Fine tuning dos parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF9mxe7y23hr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG31I7_n4RQg"
      },
      "source": [
        "### Aplicar as transformações (principais) estudadas e reestimar o modelo novamente\n",
        "* Qual o impacto das transformações?\n",
        "* A conclusão muda/mudou?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYgK6JXd3MgA"
      },
      "source": [
        "## Exercício 2 - Predicting species on IRIS dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si0rsJvu3O6O"
      },
      "source": [
        "from sklearn import datasets\n",
        "import xgboost as xgb\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zom8t4yWC_UC"
      },
      "source": [
        "## Exercício 3 - Predict Wine Quality\n",
        "> Estimar a qualidade dos vinhos, numa scala de 0–100. A seguir, a qualidade em função da escala:\n",
        "\n",
        "* 95–100 Classic: a great wine\n",
        "* 90–94 Outstanding: a wine of superior character and style\n",
        "* 85–89 Very good: a wine with special qualities\n",
        "* 80–84 Good: a solid, well-made wine\n",
        "* 75–79 Mediocre: a drinkable wine that may have minor flaws\n",
        "* 50–74 Not recommended\n",
        "\n",
        "Source: [Wine Reviews](https://www.kaggle.com/zynicide/wine-reviews)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klL2Q9Ria96n"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "\n",
        "Wine = datasets.load_wine()\n",
        "X_vinho = Wine.data\n",
        "y_vinho = Wine.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhVhSWBgGijq"
      },
      "source": [
        "## Exercício 4 - Predict Parkinson\n",
        "Source: https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVCxHqv0VBJn"
      },
      "source": [
        "## Exercício 5 - Predict survivors from Titanic tragedy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwvB8us4eKNi"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "df_titanic = sns.load_dataset('titanic')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJrT9YIXVdtx"
      },
      "source": [
        "## Exercício 6 - Predict Loan\n",
        "> Os dados devem ser obtidos diretamente da fonte: [Loan Default Prediction - Imperial College London](https://www.kaggle.com/c/loan-default-prediction/data)\n",
        "\n",
        "* [Bank Loan Default Prediction](https://medium.com/@wutianhao910/bank-loan-default-prediction-94d4902db740)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8-GVu7ZWeA8"
      },
      "source": [
        "## Exercício 7 - Predict the sales of a store.\n",
        "* [Predicting expected sales for Bigmart’s stores](https://medium.com/diogo-menezes-borges/project-1-bigmart-sale-prediction-fdc04f07dc1e)\n",
        "* Dataframes\n",
        "    * [Treinamento](https://raw.githubusercontent.com/MathMachado/DataFrames/master/Big_Mart_Sales_III_train.txt)\n",
        "    * [Validação](https://raw.githubusercontent.com/MathMachado/DataFrames/master/Big_Mart_Sales_III_test.txt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv9w86j4Wnwj"
      },
      "source": [
        "## Exercício 8 - [The Boston Housing Dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)\n",
        "> Predict the median value of owner occupied homes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HYRt8-ug1BT"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "\n",
        "Boston = datasets.load_boston()\n",
        "X_boston = Boston.data\n",
        "y_boston = Boston.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UDIaqmtXQ0T"
      },
      "source": [
        "## Exercício 9 - Predict the height or weight of a person.\n",
        "\n",
        "http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_020108_HeightsWeights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7R146nIXmMT"
      },
      "source": [
        "## Exercício 10 - Black Friday Sales Prediction - Predict purchase amount.\n",
        "\n",
        "This dataset comprises of sales transactions captured at a retail store. It’s a classic dataset to explore and expand your feature engineering skills and day to day understanding from multiple shopping experiences. This is a regression problem. The dataset has 550,069 rows and 12 columns.\n",
        "\n",
        "https://github.com/MathMachado/DataFrames/blob/master/blackfriday.zip\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ8FPbuLZlIh"
      },
      "source": [
        "## Exercício 11 - Predict the income class of US population.\n",
        "\n",
        "http://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af4NRrchgPlM"
      },
      "source": [
        "## Exercício 12 - Predicting Cancer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4LOlgZW3P40"
      },
      "source": [
        "from sklearn import datasets\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74PmpT8Ix0tD"
      },
      "source": [
        "## Exercício 13\n",
        "Source: [Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting (GBM) in Python](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY8GZMixZ9W9"
      },
      "source": [
        "## Exercício 14 - Predict Diabetes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y92t6tbOge0S"
      },
      "source": [
        "from sklearn import datasets\n",
        "Diabetes= datasets.load_diabetes()\n",
        "\n",
        "X_diabetes = Diabetes.data\n",
        "y_diabetes = Diabetes.target"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}